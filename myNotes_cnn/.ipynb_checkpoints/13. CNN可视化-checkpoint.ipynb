{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 为什么要可视化卷积核？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Justin Johnson 在课上回答的一个问题，说的很透彻：\n",
    ">Tell you what the filter is looking for. So this intuition comes from sort of template matching and inner products. That if you imagine you have some template vector. And then you imagine you computer a scaler output by taking inner product betwween your template vector and some arbitrary piece of data. Then ,the input which maximizes that activation. Under a norm constraint on the input is exactly when those two vectors match up. So, in that since that, whenever you're taking inner products, the thing causes an inner product to excite maximally is a copy of thing you are taking an inner product with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. What's going on inside ConvNet?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 First Layer: Visualize Filters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![这里写图片描述](http://img.blog.csdn.net/2018031121141115?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcGFueGlhb3hpZQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第一层的卷积核看起来是像在寻找一些简单的特征，比如边缘特征、垂直特征。但是在其他的layers，效果就没这么明显了，比较难看出在找什么了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Krizhevsky, “One weird trick for parallelizing convolutional neural networks”, arXiv 2014]\n",
    "\n",
    "[He et al, “Deep Residual Learning for Image Recognition”, CVPR 2016]\n",
    "\n",
    "[Huang et al, “Densely Connected Convolutional Networks”, CVPR 2017]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Last layer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后一层全连接层的输出(N, 4096),也就是还没有给出分类结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![这里写图片描述](http://img.blog.csdn.net/20180311212513944?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcGFueGlhb3hpZQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将通过训练后得到的4096维的向量可视化，与原来的图进行对比，会发现CNN更关注图像的特征空间，比如大象的那组图，不管是绿色的草地，还是黄土地，都能识别出是大象，说明神经网络更关注特征空间。\n",
    "\n",
    "而左边的图中使用的最近邻更关注的是像素空间，比如第二组图，白色的狗狗和背景为白色的花儿都会分为一类。这显然是不好的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![这里写图片描述](http://img.blog.csdn.net/20180311213109774?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcGFueGlhb3hpZQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用非线性降维方法[t-SNE.](http://www.jmlr.org/papers/v9/vandermaaten08a.html)\n",
    "\n",
    "[t-SNE visualization of CNN codes](https://cs.stanford.edu/people/karpathy/cnnembed/)\n",
    "\n",
    "[Yosinski et al, “Understanding Neural Networks Through Deep Visualization”, ICML DL Workshop 2014.](http://yosinski.com/media/papers/Yosinski__2015__ICML_DL__Understanding_Neural_Networks_Through_Deep_Visualization__.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Maximally Activating Patches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![这里写图片描述](http://img.blog.csdn.net/20180311214703850?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcGFueGlhb3hpZQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Springenberg et al, “Striving for Simplicity: The All Convolutional Net”, ICLR Workshop 2015]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "选取一个channel,找出图像中的某个patches，能最大激活？？没听太懂。。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4  Occlusion Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![这里写图片描述](http://img.blog.csdn.net/20180311215223446?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcGFueGlhb3hpZQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)\n",
    "\n",
    "[Zeiler and Fergus, “Visualizing and Understanding Convolutional\n",
    "Networks”, ECCV 2014]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "遮挡住其中某一部分，用图片的平均像素来代替，然后输入神经网络。具体看文章把。。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5 Saliency Maps\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180311215952140?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcGFueGlhb3hpZQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)\n",
    "[Simonyan, Vedaldi, and Zisserman, “Deep Inside Convolutional Networks: Visualising Image Classification Models\n",
    "and Saliency Maps”, ICLR Workshop 2014.]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这部分会在作业里面出现～太好了，还能做完作业才能理解。。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.6 Intermediate features via (guided) backprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![这里写图片描述](http://img.blog.csdn.net/20180311220221713?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcGFueGlhb3hpZQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Zeiler and Fergus, “Visualizing and Understanding Convolutional Networks”, ECCV 2014]\n",
    "\n",
    "[Springenberg et al, “Striving for Simplicity: The All Convolutional Net”, ICLR Workshop 2015]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.7 Visualizing CNN features: Gradient Ascent　梯度上升"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![这里写图片描述](http://img.blog.csdn.net/20180311220445183?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcGFueGlhb3hpZQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![这里写图片描述](http://img.blog.csdn.net/20180311220959135?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcGFueGlhb3hpZQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.8 Fooling Images / Adversarial Examples\n",
    "\n",
    "1. Start from an arbitrary image\n",
    "2. Pick an arbitrary class\n",
    "3. Modify the image to maximize the class\n",
    "4. Repeat until network is fooled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
