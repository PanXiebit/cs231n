{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Activations**:Nearest neighbors, Dimensionality reduction,maximal patches, occlusion\n",
    "- **Gradients**: Saliency maps, class visualization, foolingimages, feature inversion\n",
    "- **Fun**: DeepDream, Style Transfer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 为什么要可视化卷积核？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Justin Johnson 在课上回答的一个问题，说的很透彻：\n",
    ">Tell you what the filter is looking for. So this intuition comes from sort of template matching and inner products. That if you imagine you have some template vector. And then you imagine you computer a scaler output by taking inner product betwween your template vector and some arbitrary piece of data. Then ,the input which maximizes that activation. Under a norm constraint on the input is exactly when those two vectors match up. So, in that since that, whenever you're taking inner products, the thing causes an inner product to excite maximally is a copy of thing you are taking an inner product with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "个人感觉：卷积神经网络与线性分类器有异曲同工之妙，都是用权重去匹配原有的图像，在一个10分类的线性分类器中，权重矩阵每一行可代表一个类别的分类器，而卷积神经网络呢，也是用权重去匹配模版向量。不同的是，线性分类器将图像拉伸为一维向量，而卷积神经网络保留了原有图像的像素位置。并且卷积神经网络的权重，也就是卷积核一般是3x3,5x5，使得计算量大大减小，因为涉及到了参数共享和稀疏连接。从第一层卷积核，可以看出卷积核是提取对应的某一个特征，而更深层次的卷积核看不出来，这是因为多个特征进行了混合吧？再看倒数第一层的特征，你会发现它跟原有的图像类似了～～～"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. What's going on inside ConvNet?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 First Layer: Visualize Filters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![这里写图片描述](http://img.blog.csdn.net/2018031121141115?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcGFueGlhb3hpZQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第一层的卷积核看起来是像在寻找一些简单的特征，比如边缘特征、垂直特征。但是在其他的layers，效果就没这么明显了，比较难看出在找什么了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Krizhevsky, “One weird trick for parallelizing convolutional neural networks”, arXiv 2014]\n",
    "\n",
    "[He et al, “Deep Residual Learning for Image Recognition”, CVPR 2016]\n",
    "\n",
    "[Huang et al, “Densely Connected Convolutional Networks”, CVPR 2017]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Last layer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后一层全连接层的输出(N, 4096),也就是还没有给出分类结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![这里写图片描述](http://img.blog.csdn.net/20180311212513944?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcGFueGlhb3hpZQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将通过训练后得到的4096维的向量可视化，与原来的图进行对比，会发现CNN更关注图像的特征空间，比如大象的那组图，不管是绿色的草地，还是黄土地，都能识别出是大象，说明神经网络更关注特征空间。\n",
    "\n",
    "而左边的图中使用的最近邻更关注的是像素空间，比如第二组图，白色的狗狗和背景为白色的花儿都会分为一类。这显然是不好的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![这里写图片描述](http://img.blog.csdn.net/20180311213109774?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcGFueGlhb3hpZQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用非线性降维方法[t-SNE.](http://www.jmlr.org/papers/v9/vandermaaten08a.html)\n",
    "\n",
    "[t-SNE visualization of CNN codes](https://cs.stanford.edu/people/karpathy/cnnembed/)\n",
    "\n",
    "[Yosinski et al, “Understanding Neural Networks Through Deep Visualization”, ICML DL Workshop 2014.](http://yosinski.com/media/papers/Yosinski__2015__ICML_DL__Understanding_Neural_Networks_Through_Deep_Visualization__.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing Activations\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180311222057662?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcGFueGlhb3hpZQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)\n",
    "http://yosinski.com/deepvis 真特么cool!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Maximally Activating Patches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![这里写图片描述](http://img.blog.csdn.net/20180311214703850?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcGFueGlhb3hpZQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Springenberg et al, “Striving for Simplicity: The All Convolutional Net”, ICLR Workshop 2015]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "选取一个channel,找出图像中的某个patches，能最大激活？？没听太懂。。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4  Occlusion Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![这里写图片描述](http://img.blog.csdn.net/20180311215223446?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcGFueGlhb3hpZQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)\n",
    "\n",
    "[Zeiler and Fergus, “Visualizing and Understanding Convolutional\n",
    "Networks”, ECCV 2014]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "遮挡住其中某一部分，用图片的平均像素来代替，然后输入神经网络。具体看文章把。。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5 Saliency Maps\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180311215952140?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcGFueGlhb3hpZQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)\n",
    "[Simonyan, Vedaldi, and Zisserman, “Deep Inside Convolutional Networks: Visualising Image Classification Models\n",
    "and Saliency Maps”, ICLR Workshop 2014.]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这部分会在作业里面出现～太好了，还能做完作业才能理解。。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.6 Intermediate features via (guided) backprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![这里写图片描述](http://img.blog.csdn.net/20180311220221713?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcGFueGlhb3hpZQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Zeiler and Fergus, “Visualizing and Understanding Convolutional Networks”, ECCV 2014]\n",
    "\n",
    "[Springenberg et al, “Striving for Simplicity: The All Convolutional Net”, ICLR Workshop 2015]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.7 Visualizing CNN features: Gradient Ascent　梯度上升"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![这里写图片描述](http://img.blog.csdn.net/20180311220445183?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcGFueGlhb3hpZQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![这里写图片描述](http://img.blog.csdn.net/20180311220959135?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcGFueGlhb3hpZQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.8 Fooling Images / Adversarial Examples\n",
    "\n",
    "1. Start from an arbitrary image\n",
    "2. Pick an arbitrary class\n",
    "3. Modify the image to maximize the class\n",
    "4. Repeat until network is fooled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.9  DeepDream\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180314205105863?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcGFueGlhb3hpZQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n",
    "\n",
    "google blog:https://research.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 风格转移style transfer\n",
    "- 特征反演feature inversion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![这里写图片描述](http://img.blog.csdn.net/20180314205630665?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcGFueGlhb3hpZQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180314205645277?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcGFueGlhb3hpZQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n",
    "\n",
    "网络层次越深，更多是的保留语义信息(semantic information)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Mahendran and Vedaldi, “Understanding Deep Image Representations by Inverting Them”, CVPR 2015]\n",
    "\n",
    "[Figure from Johnson, Alahi, and Fei-Fei, “Perceptual Losses for Real-Time Style Transfer and Super-Resolution”, ECCV 2016. Copyright Springer, 2016.\n",
    "Reproduced for educational purposes.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.10 纹理合成 Texture Synthesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![这里写图片描述](http://img.blog.csdn.net/20180314213013524?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcGFueGlhb3hpZQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![这里写图片描述](http://img.blog.csdn.net/20180314213206536?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcGFueGlhb3hpZQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Gatys, Ecker, and Bethge, “Texture Synthesis Using Convolutional Neural Networks”, NIPS 2015]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.11 Style Transfer\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180314213806140?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcGFueGlhb3hpZQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/jcjohnson/fast-neural-style\n",
    "\n",
    "[Johnson, Alahi, and Fei-Fei, “Perceptual Losses for Real-Time Style Transfer and Super-Resolution”, ECCV 2016]\n",
    "\n",
    "[Dumoulin, Shlens, and Kudlur, “A Learned Representation for Artistic Style”, ICLR 2017.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 总结：many methods for understanding CNN representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Activations**:Nearest neighbors, Dimensionality reduction,maximal patches, occlusion\n",
    "- **Gradients**: Saliency maps, class visualization, foolingimages, feature inversion\n",
    "- **Fun**: DeepDream, Style Transfer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
