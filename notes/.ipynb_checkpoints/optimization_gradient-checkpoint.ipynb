{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - 损失函数可视化\n",
    " - 最优化\n",
    "\n",
    "前面介绍了两部分：\n",
    " 1. 评分函数：将原始图像像素映射为分类评分值\n",
    " 2. 损失函数：根据评分函数和训练集图像数据实际分类的一致性，衡量某个具体参数集的质量好坏。\n",
    "那么寻找到能使损失函数值最小化的参数的过程就是最优化 Optimization。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "损失函数可视化\n",
    "====\n",
    "损失函数L可以看作是权重W的函数，在CIFAR-10中一个分类器的权重矩阵大小是[10,3073]，即$L(W_1,W_2,....,W10)$，对其中某一个分类器$W_i$有3073个参数，想要得到$L$关于$W_i$的可视化很难。\n",
    "但是方法还是有的，随机生成一个权重矩阵W，并沿着此方向计算损失值，$L(W+\\alpha W_1)$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[具体参考这里！](https://zhuanlan.zhihu.com/p/21360434?refer=intelligentunit)\n",
    "[还有这样一篇paper！](https://arxiv.org/abs/1712.09913)\n",
    "\n",
    "总而言之，就是将高维空间压缩到二维，$W_i[1,3073]$转换到$[1,1]$然后在此基础上，画出loss关于它的值。\n",
    "如果是压缩到三维，就是[1,3073]->[1,2]，那么完整的loss就是这个形状的3073/2*10维的版本。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最优化Optimization\n",
    "==\n",
    "\n",
    "在数学上我们已经知道loss下降最快的方向就是梯度方向（gradient）。\n",
    "\n",
    "**有限差值法计算梯度**：\n",
    "\n",
    "公式：$\\frac{df(x)}{dx} = \\lim_{h\\to 0}\\frac{f(x+h)-f(x)}{h}$\n",
    "下面代码是一个输入为函数f和向量x，计算f的梯度的通用函数，它返回函数f在点x处的梯度："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eval_numerical_gradient(f, x):\n",
    "  \"\"\"  \n",
    "  一个f在x处的数值梯度法的简单实现\n",
    "  - f是只有一个参数的函数\n",
    "  - x是计算梯度的点\n",
    "  \"\"\" \n",
    "\n",
    "  fx = f(x) # 在原点计算函数值\n",
    "  grad = np.zeros(x.shape)   ##\n",
    "  h = 0.00001\n",
    "\n",
    "  # 对x中所有的索引进行迭代\n",
    "  it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "  while not it.finished:\n",
    "\n",
    "    # 计算x+h处的函数值\n",
    "    ix = it.multi_index\n",
    "    old_value = x[ix]\n",
    "    x[ix] = old_value + h # 增加h\n",
    "    fxh = f(x) # 计算f(x + h)\n",
    "    x[ix] = old_value # 存到前一个值中 (非常重要)\n",
    "\n",
    "    # 计算偏导数\n",
    "    grad[ix] = (fxh - fx) / h # 坡度\n",
    "    it.iternext() # 到下个维度\n",
    "\n",
    "  return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以使用上面这个公式来计算任意函数在任意点上的梯度。下面计算权重空间中的某些随机点上，CIFAR-10损失函数的梯度："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cs231n'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-b936c9a188dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcs231n\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massignment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcs231n\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_CIFAR10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcs231n\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massignment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcs231n\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifiers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msoftmax_loss_vectorized\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cs231n'"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from assignment1.cs231n.data_utils import load_CIFAR10\n",
    "from assignment1.cs231n.classifiers.softmax import softmax_loss_vectorized\n",
    "\n",
    "##原始数据\n",
    "cifar10_dir = 'cs231n/datasets/cifar-10-batches-py'\n",
    "X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "\n",
    "##根据softmax损失函数据算loss,这个函数里面有用微分法计算梯度，但我们只取loss\n",
    "def CIFAR_loss_fun(W):\n",
    "    loss,dw = softmax_loss_vectorized(W, X_train, y_train, 0.000005)\n",
    "    return  loss\n",
    "\n",
    "W = np.random.randn(3073, 10) * 0.0001  ##随机权重向量\n",
    "df = eval_numerical_gradient(CIFAR_loss_fun, W) ##计算权重空间下任意点关于loss的梯度\n",
    "\n",
    "loss_original = CIFAR_loss_fun(W)  ##初始损失值\n",
    "print(\"original loss: %f\"%(loss_original,))\n",
    "\n",
    "#查看不同步长的效果\n",
    "for step_size_log in [-10,-9,-8,-7,-6,-5,-4,-3,-2,-1]:\n",
    "    step_size = 10**step_size_log\n",
    "    W_new = W-step_size*df\n",
    "    loss_new = CIFAR_loss_fun(W_new)\n",
    "    print(\"for step size %f new loss: %f\" % (step_size, loss_new))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
