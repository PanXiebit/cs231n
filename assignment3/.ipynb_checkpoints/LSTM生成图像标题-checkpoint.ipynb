{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这篇文章是将cs231n中LSTM_Caption重新敲了一遍，所有的模块放在一起，以便于系统的理解整个过程。目的是读懂其中的每一行代码，即使是课程中已经帮你写好了的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As usual, a bit of setup\n",
    "from __future__ import print_function\n",
    "import time, os, json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 自动加载模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 下载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_captions <class 'numpy.ndarray'> (400135, 17) int32\n",
      "train_image_idxs <class 'numpy.ndarray'> (400135,) int32\n",
      "val_captions <class 'numpy.ndarray'> (195954, 17) int32\n",
      "val_image_idxs <class 'numpy.ndarray'> (195954,) int32\n",
      "train_features <class 'numpy.ndarray'> (82783, 512) float32\n",
      "val_features <class 'numpy.ndarray'> (40504, 512) float32\n",
      "idx_to_word <class 'list'> 1004\n",
      "word_to_idx <class 'dict'> 1004\n",
      "train_urls <class 'numpy.ndarray'> (82783,) <U63\n",
      "val_urls <class 'numpy.ndarray'> (40504,) <U63\n"
     ]
    }
   ],
   "source": [
    "from builtins import range\n",
    "import os, json\n",
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "BASE_DIR = 'cs231n/datasets/coco_captioning'\n",
    "\n",
    "def load_coco_data(base_dir=BASE_DIR,\n",
    "                   max_train=None,\n",
    "                   pca_features=True):\n",
    "    \"\"\"\n",
    "    从本地磁盘获取数据，生成的数据集包括：(captions, features, URLs, and vocabulary) \n",
    "    - captions:图像标题经过分词后得到的\n",
    "    - features:图像特征\n",
    "    - URLs:图像对应的网络链接。因为图片占内存太大，因而只存储其链接\n",
    "    - vovabulary:词典\n",
    "    \n",
    "    输入:\n",
    "    - pca_features:　图像特征是否经过降维处理。图像是通过VGG-16 network在ImageNet训练过后提取的特征。\n",
    "    　这些图像特征分别存储在train2014_vgg16_fc7.h5 和 val2014_vgg16_fc7.h5 文件中。通过pca降维\n",
    "      处理后的特征放在train2014_vgg16_fc7_pca.h5 和 val2014_vgg16_fc7_pca.h5文件中。\n",
    "    - max_train:\n",
    "    \"\"\"\n",
    "    \n",
    "    data = {}\n",
    "    caption_file = os.path.join(base_dir, 'coco2014_captions.h5')\n",
    "    \n",
    "    # 获取图像标题数据，注意里面存储的是word对应的Integer ID\n",
    "    # 请记住h5py.File类似python的词典对象,详细查看链接:http://blog.csdn.net/yudf2010/article/details/50353292\n",
    "    with h5py.File(caption_file, 'r') as f:\n",
    "        assert(list(f.keys())==['train_captions', 'train_image_idxs', 'val_captions', 'val_image_idxs'])\n",
    "        for k, v in f.items(): \n",
    "            data[k] = np.asarray(v)\n",
    "    \n",
    "    # 获取图像特征:\n",
    "    if pca_features:\n",
    "        train_feat_file = os.path.join(base_dir, 'train2014_vgg16_fc7_pca.h5')\n",
    "    else:\n",
    "        train_feat_file = os.path.join(base_dir, 'train2014_vgg16_fc7.h5')\n",
    "    with h5py.File(train_feat_file, 'r') as f:\n",
    "        assert(list(f.keys())==['features'])\n",
    "        data['train_features'] = np.asarray(f['features'])    \n",
    "    if pca_features:\n",
    "        val_feat_file = os.path.join(base_dir, 'val2014_vgg16_fc7_pca.h5')\n",
    "    else:\n",
    "        val_feat_file = os.path.join(base_dir, 'val2014_vgg16_fc7.h5')\n",
    "    with h5py.File(val_feat_file, 'r') as f:\n",
    "        data['val_features'] = np.asarray(f['features'])\n",
    "    \n",
    "    # 获取词典\n",
    "    # 关于json详细可查看python3-cookbook:http://python3-cookbook.readthedocs.io/zh_CN/latest/c06/p02_read-write_json_data.html\n",
    "    dict_file = os.path.join(base_dir, 'coco2014_vocab.json')\n",
    "    with open(dict_file, 'r') as f:\n",
    "        dict_data = json.load(f)\n",
    "        for k, v in dict_data.items():\n",
    "            data[k] = v\n",
    "    \n",
    "    # 获取图像链接\n",
    "    train_url_file = os.path.join(base_dir, 'train2014_urls.txt')\n",
    "    with open(train_url_file, 'r') as f:\n",
    "        train_urls = np.asarray([line.strip() for line in f])\n",
    "    data['train_urls'] = train_urls\n",
    "    val_url_file = os.path.join(base_dir, 'val2014_urls.txt')\n",
    "    with open(val_url_file, 'r') as f:\n",
    "        val_urls = np.asarray([line.strip() for line in f])\n",
    "    data['val_urls'] = val_urls\n",
    "    \n",
    "    # 对训练数据进行采样，随机选取图片以及其对应的标题\n",
    "    if max_train is not None:   ## max_train表示训练集样本数\n",
    "        num_train = data['train_captions'].shape[0]\n",
    "        mask = np.random.randint(num_train, size=max_train)\n",
    "        data['train_captions'] = data['train_captions'][mask]\n",
    "        data['train_image_idxs'] = data['train_image_idxs'][mask]\n",
    "    \n",
    "    return data\n",
    "\n",
    "data = load_coco_data(BASE_DIR)\n",
    "for k, v in data.items():\n",
    "    if type(v) == np.ndarray:\n",
    "        print(k, type(v), v.shape, v.dtype)\n",
    "    else:\n",
    "        print(k, type(v), len(v))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1  LSTM step forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个图是来自Ng的courser课程作业中的截图。虽然Ng的课程作业相比cs231n的作业太简单了，完全的手把手的教。。感觉不太好，很多东西模块化了看不到，不知道具体细节。但是Ng的课讲的确实是好！而且里面的图画的也很好，就借鉴过来了～但符号标注跟cs231n不太一样，不过能理解就没问题！\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180309113952226?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcGFueGlhb3hpZQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">LSTM相比vanilla RNN，由于重复矩阵乘法导致消失和爆炸梯度，因此vanilla RNN可能很难训练长序列。LSTM通过用选通机制代替vanilla RNN的简单更新规则来解决这个问题。\n",
    "\n",
    ">代码与上图中的符号不太一致。图中隐藏层用$a^{<t-1>}，a^{<t>}$表示，而在代码中用prev_h和next_h表示。代码中的a表示为经过激活函数的gate.\n",
    "然后经过激活函数:ai表示更新门$\\sigma_u$, af表示遗忘门$\\sigma_f$, ao表示输出门$\\sigma_o$，ag要更新的memory cell即$\\tilde c^{<t>}$，next_c表示新的memory cell状态。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    A numerically stable version of the logistic sigmoid function.\n",
    "    \"\"\"\n",
    "    pos_mask = (x >= 0)\n",
    "    neg_mask = (x < 0)\n",
    "    z = np.zeros_like(x)\n",
    "    z[pos_mask] = np.exp(-x[pos_mask])\n",
    "    z[neg_mask] = np.exp(x[neg_mask])  ###　大于０的加符号，小于０的不加。。为啥？\n",
    "    top = np.ones_like(x)\n",
    "    top[neg_mask] = z[neg_mask]\n",
    "    return top / (1 + z)\n",
    "\n",
    "def lstm_step_forward(x, prev_h, prev_c, Wx, Wh, b):\n",
    "    \"\"\"\n",
    "    Forward pass for a single timestep of an LSTM.\n",
    "\n",
    "    The input data has dimension D, the hidden state has dimension H, and we use\n",
    "    a minibatch size of N.\n",
    "\n",
    "    Inputs:\n",
    "    - x: Input data, of shape (N, D)\n",
    "    - prev_h: Previous hidden state, of shape (N, H)\n",
    "    - prev_c: previous cell state, of shape (N, H)\n",
    "    - Wx: Input-to-hidden weights, of shape (D, 4H)\n",
    "    - Wh: Hidden-to-hidden weights, of shape (H, 4H)\n",
    "    - b: Biases, of shape (4H,)\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - next_h: Next hidden state, of shape (N, H)\n",
    "    - next_c: Next cell state, of shape (N, H)\n",
    "    - cache: Tuple of values needed for backward pass.\n",
    "    \"\"\"\n",
    "    next_h, next_c, cache = None, None, None\n",
    "    #############################################################################\n",
    "    # TODO: Implement the forward pass for a single timestep of an LSTM.        #\n",
    "    # You may want to use the numerically stable sigmoid implementation above.  #\n",
    "    #############################################################################\n",
    "    N, H = prev_h.shape\n",
    "    a = x.dot(Wx) + prev_h.dot(Wh) + b  # (N, 4H)\n",
    "    \n",
    "    # compute gate\n",
    "    ai = a[:, :H]\n",
    "    af = a[:, H:2*H]\n",
    "    ao = a[:, 2*H:3*H]\n",
    "    ag = a[:, 3*H:]\n",
    "    gate_i = sigmoid(ai)        # update gate\n",
    "    gate_f = sigmoid(af)        # forget gate\n",
    "    gate_o = sigmoid(ao)        # output gate\n",
    "    gate_g = np.tanh(ag)        # c_tilde\n",
    "    \n",
    "    next_c = gate_i * gate_g + gate_f * prev_c   # new cell state (N, H)\n",
    "    next_h = gate_o * np.tanh(next_c)\n",
    "    cache = (x, prev_h, prev_c, Wx, Wh, b, next_c, ai, af, ao, ag, gate_i, gate_f, gate_o, gate_g)\n",
    "    ##############################################################################\n",
    "    #                               END OF YOUR CODE                             #\n",
    "    ##############################################################################\n",
    "\n",
    "    return next_h, next_c, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2  LSTM step backward\n",
    "\n",
    ">通过链式求导一步步计算即可。但需要注意的一点是对于lstm_step_backward()这个函数其输入dnext_h是包括损失函数求导得到的梯度加上下一层回流的梯度。\n",
    "\n",
    ">同样的道理记忆细胞也会forward到下一层，并参与到这一层输出损失函数。因此其梯度也包括两部分。这个函数中dnext_c是下一层回流的梯度，因此需要加上当前层损失函数求导得到的梯度。\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180309173626389?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcGFueGlhb3hpZQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个图是cs231n中的图，$\\odot$表示点乘。这个图跟上一个图是一样的，不过门机制中的激活函数没有表示出来，都画得棒棒的～～"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$tanh(x) = \\dfrac{sinh(x)}{cosh(x)} = \\dfrac{e^x-e^{-x}}{e^x+e^{-x}}\\quad$\n",
    "求导可得：$\\dfrac{\\partial tanh(x)}{\\partial x} = 1-(tanh(x))^2$\n",
    "\n",
    "$\\sigma (x) = \\dfrac{1}{1+e^{-x}}\\quad$ \n",
    "求导可得：$\\dfrac{\\partial \\sigma(x)}{\\partial x} = \\sigma (x)(1-\\sigma (x))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_step_backward(dnext_h, dnext_c, cache):\n",
    "    \"\"\"\n",
    "    Backward pass for a single timestep of an LSTM.\n",
    "\n",
    "    Inputs:\n",
    "    - dnext_h: Gradients of next hidden state, of shape (N, H)\n",
    "    - dnext_c: Gradients of next cell state, of shape (N, H)\n",
    "    - cache: Values from the forward pass\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - dx: Gradient of input data, of shape (N, D)\n",
    "    - dprev_h: Gradient of previous hidden state, of shape (N, H)\n",
    "    - dprev_c: Gradient of previous cell state, of shape (N, H)\n",
    "    - dWx: Gradient of input-to-hidden weights, of shape (D, 4H)\n",
    "    - dWh: Gradient of hidden-to-hidden weights, of shape (H, 4H)\n",
    "    - db: Gradient of biases, of shape (4H,)\n",
    "    \"\"\"\n",
    "    dx, dh, dc, dWx, dWh, db = None, None, None, None, None, None\n",
    "    #############################################################################\n",
    "    # TODO: Implement the backward pass for a single timestep of an LSTM.       #\n",
    "    #                                                                           #\n",
    "    # HINT: For sigmoid and tanh you can compute local derivatives in terms of  #\n",
    "    # the output value from the nonlinearity.                                   #\n",
    "    #############################################################################\n",
    "    N, H = dnext_h.shape\n",
    "    # unroll cache\n",
    "    x, prev_h, prev_c, Wx, Wh, b, next_c, ai, af, ao, ag, gate_i, gate_f, gate_o, gate_g = cache\n",
    "    \n",
    "    dgate_o = dnext_h * np.tanh(next_c)\n",
    "    # 除了在这个时间步生成损失函数回流的梯度dh/dnext_c，还有上一层cell memory回流的梯度dnext_c\n",
    "    dnext_c += dnext_h * gate_o * (1 - np.tanh(next_c)**2)\n",
    "    \n",
    "    dgate_i = dnext_c * gate_g\n",
    "    dgate_f = dnext_c * prev_c\n",
    "    dgate_g = dnext_c * gate_i\n",
    "    dprev_c = dnext_c * gate_f    # dprev_c  (N, H)\n",
    "       \n",
    "    dai = gate_i * (1 - gate_i) * dgate_i\n",
    "    daf = gate_f * (1 - gate_f) * dgate_f\n",
    "    dao = gate_o * (1 - gate_o) * dgate_o\n",
    "    dag = (1 - gate_g**2) * dgate_g\n",
    "    \n",
    "    da = np.hstack((dai, daf, dao, dag)) # (N ,4H)\n",
    "    assert(da.shape == (N, 4*H))\n",
    "    \n",
    "    dx = da.dot(Wx.T)         # dx (N, D)\n",
    "    dWx = x.T.dot(da)         # dWx  (D, 4H)\n",
    "    dprev_h = da.dot(Wh.T)    # dprev_h  (N, H)\n",
    "    dWh = prev_h.T.dot(da)    # dWh (H, 4H)    \n",
    "    db = np.sum(da, axis=0)   # db (1, 4H)\n",
    "    pass\n",
    "    ##############################################################################\n",
    "    #                               END OF YOUR CODE                             #\n",
    "    ##############################################################################\n",
    "\n",
    "    return dx, dprev_h, dprev_c, dWx, dWh, db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![这里写图片描述](http://img.blog.csdn.net/20180309162831167?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcGFueGlhb3hpZQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180309172734983?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcGFueGlhb3hpZQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n",
    "\n",
    "这两图从两个角度表示了完成的RNN过程，上面的一个更宏观。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 LSTM forward\n",
    "\n",
    ">前向传播都是比较简单的，这里需要注意的是记忆细胞的初始值设为0, h0这里是作为输入。因为这篇文章是针对图像标注的，之后图像特征会赋值给h0。如果是其他模型一般也是设为0．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_forward(x, h0, Wx, Wh, b):\n",
    "    \"\"\"\n",
    "    Forward pass for an LSTM over an entire sequence of data. We assume an input\n",
    "    sequence composed of T vectors, each of dimension D. The LSTM uses a hidden\n",
    "    size of H, and we work over a minibatch containing N sequences. After running\n",
    "    the LSTM forward, we return the hidden states for all timesteps.\n",
    "\n",
    "    Note that the initial cell state is passed as input, but the initial cell\n",
    "    state is set to zero. Also note that the cell state is not returned; it is\n",
    "    an internal variable to the LSTM and is not accessed from outside.\n",
    "\n",
    "    Inputs:\n",
    "    - x: Input data of shape (N, T, D)\n",
    "    - h0: Initial hidden state of shape (N, H)\n",
    "    - Wx: Weights for input-to-hidden connections, of shape (D, 4H)\n",
    "    - Wh: Weights for hidden-to-hidden connections, of shape (H, 4H)\n",
    "    - b: Biases of shape (4H,)\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - h: Hidden states for all timesteps of all sequences, of shape (N, T, H)\n",
    "    - cache: Values needed for the backward pass.\n",
    "    \"\"\"\n",
    "    h, cache = None, None\n",
    "    #############################################################################\n",
    "    # TODO: Implement the forward pass for an LSTM over an entire timeseries.   #\n",
    "    # You should use the lstm_step_forward function that you just defined.      #\n",
    "    #############################################################################\n",
    "    N, T, D = x.shape\n",
    "    N, H = h0.shape\n",
    "    h = np.zeros((N, T, H))\n",
    "    c = np.zeros((N, T, H))\n",
    "    cache = []\n",
    "    next_h = h0\n",
    "    next_c = np.zeros((N ,H))\n",
    "    for t in range(T):\n",
    "        next_h, next_c, cache_t = lstm_step_forward(x[:,t,:], next_h, next_c, Wx, Wh, b) # 权重参数共享\n",
    "        h[:, t, :] = next_h\n",
    "        c[:, t, :] = next_c\n",
    "        cache.append(cache_t)    \n",
    "    ##############################################################################\n",
    "    #                               END OF YOUR CODE                             #\n",
    "    ##############################################################################\n",
    "\n",
    "    return h, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 LSTM backward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">完整的整个时间序列上的反向传播.需要注意的是参数dh是指的通过对每个时间步损失函数求导得到的(N,T,H)，显然不包括下一层回流的梯度，因为你压根不知道呀～而在lstm_step_backward中dnext_h没有累加，因为它只考虑了一个时间步，故其dh只需要损失函数回流的即可。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_backward(dh, cache):\n",
    "    \"\"\"\n",
    "    Backward pass for an LSTM over an entire sequence of data.]\n",
    "\n",
    "    Inputs:\n",
    "    - dh: Upstream gradients of hidden states, of shape (N, T, H)  ###　损失函数得到的梯度\n",
    "    - cache: Values from the forward pass\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - dx: Gradient of input data of shape (N, T, D)\n",
    "    - dh0: Gradient of initial hidden state of shape (N, H)\n",
    "    - dWx: Gradient of input-to-hidden weight matrix of shape (D, 4H)\n",
    "    - dWh: Gradient of hidden-to-hidden weight matrix of shape (H, 4H)\n",
    "    - db: Gradient of biases, of shape (4H,)\n",
    "    \"\"\"\n",
    "    dx, dh0, dWx, dWh, db = None, None, None, None, None\n",
    "    #############################################################################\n",
    "    # TODO: Implement the backward pass for an LSTM over an entire timeseries.  #\n",
    "    # You should use the lstm_step_backward function that you just defined.     #\n",
    "    #############################################################################\n",
    "    \n",
    "    assert(len(cache[0])==15)\n",
    "    x, prev_h, prev_c, Wx, Wh, b, next_c, ai, af, ao, ag, gate_i, gate_f, gate_o, gate_g = cache[0]\n",
    "    N, T, H = dh.shape\n",
    "    N, D = x.shape\n",
    "    \n",
    "    # initilizate\n",
    "    dx = np.zeros((N, T, D))\n",
    "    dh0 = np.zeros((N, H))\n",
    "    dWx = np.zeros((D, 4*H))\n",
    "    dWh = np.zeros((H, 4*H))\n",
    "    db = np.zeros((4*H,))\n",
    "    \n",
    "    # 隐藏层迭代的\n",
    "    dnext_h = np.zeros((N, H))\n",
    "    dnext_c = np.zeros_like(dnext_h)\n",
    "    \n",
    "    for k in range(T):\n",
    "        t = T-1-k\n",
    "        # 隐藏层h的梯度不仅来源于损失函数计算得到的dh[:,t,:]，而且有上一层回流的梯度dnext_h\n",
    "        # 记忆细胞也是这样，但在step backward中已经考虑到了两种回流\n",
    "        dnext_h += dh[:,t,:]\n",
    "        dx_t, dnext_h, dnext_c, dWx_t, dWh_t, db_t = lstm_step_backward(dnext_h, dnext_c, cache[t])\n",
    "        dx[:,t,:] = dx_t\n",
    "        # 参数是共享的\n",
    "        dWx += dWx_t\n",
    "        dWh += dWh_t\n",
    "        db += db_t\n",
    "        \n",
    "    dh0 = dnext_h\n",
    "    ##############################################################################\n",
    "    #                               END OF YOUR CODE                             #\n",
    "    ##############################################################################\n",
    "\n",
    "    return dx, dh0, dWx, dWh, db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. LSTM Caption Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">前面的部分只是考虑了隐藏层。还需要从训练数据到隐藏的输入以及从隐藏层的输出到损失函数loss，才能构成完整的模型。\n",
    "\n",
    ">整个结构：\n",
    "- 全连接层：图像特征提取，作为RNN的隐藏层的第一个输入h0。(N, input_dim)-> (N, H)\n",
    "- 词向量层：将原始标题数据转化为对应的词向量.  (N, T)->(N, T, D)\n",
    "- rnn层：vanilla RNN or LSTM.  (N, T, D)->(N, T, H)\n",
    "- 全连接层：从隐藏层的输出到每个时间步的输出在词典V中所有词的得分。 (N, T, H)->(N, T, V)\n",
    "- 损失函数：计算loss,并返回梯度shape=(N ,T, V)然后进行反向传播～\n",
    "\n",
    ">需要注意的是：\n",
    "训练和测试时是不一样的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1  词向量层\n",
    "原始captions数据(N,T)转化为隐藏层的输入(N, T, D)  D是词向量的维度～\n",
    "\n",
    "重点是：broadcasting 和 np.add,at() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_embedding_forward(x, W):\n",
    "    \"\"\"\n",
    "    Forward pass for word embeddings. We operate on minibatches of size N where\n",
    "    each sequence has length T. We assume a vocabulary of V words, assigning each\n",
    "    to a vector of dimension D.\n",
    "\n",
    "    Inputs:\n",
    "    - x: Integer array of shape (N, T) giving indices of words. Each element idx\n",
    "      of x must be in the range 0 <= idx < V.\n",
    "    - W: Weight matrix of shape (V, D) giving word vectors for all words.　### 预训练好的词向量\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - out: Array of shape (N, T, D) giving word vectors for all input words.\n",
    "    - cache: Values needed for the backward pass\n",
    "    \"\"\"\n",
    "    out, cache = None, None\n",
    "    out = W[x,:]   ##numpy的广播机制，将x的每一个元素带进去，然后取对应的词向量\n",
    "    cache = (x, W)\n",
    "    return out, cache\n",
    "\n",
    "def word_embedding_backward(dout, cache):\n",
    "    \"\"\"\n",
    "    Backward pass for word embeddings. We cannot back-propagate into the words\n",
    "    since they are integers, so we only return gradient for the word embedding\n",
    "    matrix.\n",
    "\n",
    "    HINT: Look up the function np.add.at\n",
    "\n",
    "    Inputs:\n",
    "    - dout: Upstream gradients of shape (N, T, D)\n",
    "    - cache: Values from the forward pass\n",
    "\n",
    "    Returns:\n",
    "    - dW: Gradient of word embedding matrix, of shape (V, D).\n",
    "    \"\"\"\n",
    "    dW = None\n",
    "    x, W = cache    # x.shape=(N, T)\n",
    "    dW=np.zeros_like(W) # W.shape=(V, D)\n",
    "    # 在x指定的位置将dout加到dW上\n",
    "    np.add.at(dW, x, dout) # dout.shape(N, T, D)\n",
    "    return dW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 rnn层\n",
    "\n",
    "就是前面lstm的前、反向传播。隐藏层：从输入(N, T, D)到输出(N, T, H)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 temporal_affine层\n",
    "从(N, T, H)到(N, T, V),表示每个时间步的输出在词典V中所有词的得分，这个符号与整体是一致的。下面的代码中符号不太准确。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def temporal_affine_forward(x, w, b):\n",
    "    \"\"\"\n",
    "    Forward pass for a temporal affine layer. The input is a set of D-dimensional\n",
    "    vectors arranged into a minibatch of N timeseries, each of length T. We use\n",
    "    an affine function to transform each of those vectors into a new vector of\n",
    "    dimension M.\n",
    "\n",
    "    Inputs:\n",
    "    - x: Input data of shape (N, T, D)  ##　时间t时刻对应的minibatch个词向量(N, T, D)\n",
    "    - w: Weights of shape (D, M)\n",
    "    - b: Biases of shape (M,)\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - out: Output data of shape (N, T, M)\n",
    "    - cache: Values needed for the backward pass\n",
    "    \"\"\"\n",
    "    N, T, D = x.shape\n",
    "    M = b.shape[0]\n",
    "    out = x.reshape(N * T, D).dot(w).reshape(N, T, M) + b\n",
    "    cache = x, w, b, out\n",
    "    return out, cache\n",
    "\n",
    "def temporal_affine_backward(dout, cache):\n",
    "    \"\"\"\n",
    "    Backward pass for temporal affine layer.\n",
    "\n",
    "    Input:\n",
    "    - dout: Upstream gradients of shape (N, T, M)\n",
    "    - cache: Values from forward pass\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - dx: Gradient of input, of shape (N, T, D)\n",
    "    - dw: Gradient of weights, of shape (D, M)\n",
    "    - db: Gradient of biases, of shape (M,)\n",
    "    \"\"\"\n",
    "    x, w, b, out = cache\n",
    "    N, T, D = x.shape\n",
    "    M = b.shape[0]\n",
    "\n",
    "    dx = dout.reshape(N * T, M).dot(w.T).reshape(N, T, D)\n",
    "    dw = dout.reshape(N * T, M).T.dot(x.reshape(N * T, D)).T\n",
    "    db = dout.sum(axis=(0, 1))\n",
    "\n",
    "    return dx, dw, db\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 temporal_softmax层\n",
    "\n",
    "损失函数：计算loss,并返回梯度shape=(N ,T, V)然后进行反向传播～\n",
    "\n",
    "其中序列长度为T，但不是所有的caption长度都满足，因此不足的用NULL补充，尽管在训练时，这些时间步也会得到score,但真实值并不是NULL，因此不需要计算loss.　用mask解决这个问题,mask是(N, T, D)由0,1构成\n",
    "```pyhton \n",
    "mask = (captions_out != self._null)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def temporal_softmax_loss(x, y, mask, verbose=False):\n",
    "    \"\"\"\n",
    "    A temporal version of softmax loss for use in RNNs. We assume that we are\n",
    "    making predictions over a vocabulary of size V for each timestep of a\n",
    "    timeseries of length T, over a minibatch of size N. The input x gives scores\n",
    "    for all vocabulary elements at all timesteps, and y gives the indices of the\n",
    "    ground-truth element at each timestep. We use a cross-entropy loss at each\n",
    "    timestep, summing the loss over all timesteps and averaging across the\n",
    "    minibatch.\n",
    "\n",
    "    As an additional complication, we may want to ignore the model output at some\n",
    "    timesteps, since sequences of different length may have been combined into a\n",
    "    minibatch and padded with NULL tokens. The optional mask argument tells us\n",
    "    which elements should contribute to the loss.\n",
    "\n",
    "    Inputs:\n",
    "    - x: Input scores, of shape (N, T, V)\n",
    "    - y: Ground-truth indices, of shape (N, T) where each element is in the range\n",
    "         0 <= y[i, t] < V\n",
    "    - mask: Boolean array of shape (N, T) where mask[i, t] tells whether or not\n",
    "      the scores at x[i, t] should contribute to the loss.\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - loss: Scalar giving loss\n",
    "    - dx: Gradient of loss with respect to scores x.\n",
    "    \"\"\"\n",
    "\n",
    "    N, T, V = x.shape\n",
    "\n",
    "    x_flat = x.reshape(N * T, V)    # 类似于图像一样，也要拉长～计算每个词(None,V)与真实值的交叉熵损失loss\n",
    "    y_flat = y.reshape(N * T)\n",
    "    mask_flat = mask.reshape(N * T)   # (N ,T, D)->(N*T, D)\n",
    "\n",
    "    probs = np.exp(x_flat - np.max(x_flat, axis=1, keepdims=True))  # 避免数值溢出\n",
    "    probs /= np.sum(probs, axis=1, keepdims=True)\n",
    "    loss = -np.sum(mask_flat * np.log(probs[np.arange(N * T), y_flat])) / N\n",
    "    \n",
    "    dx_flat = probs.copy()\n",
    "    dx_flat[np.arange(N * T), y_flat] -= 1\n",
    "    dx_flat /= N\n",
    "    dx_flat *= mask_flat[:, None]   # \n",
    "\n",
    "    if verbose: print('dx_flat: ', dx_flat.shape)\n",
    "\n",
    "    dx = dx_flat.reshape(N, T, V)  # 每个时间步对应的梯度\n",
    "\n",
    "    return loss, dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 caption model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CaptioningRNN(object):\n",
    "    \"\"\"\n",
    "    A CaptioningRNN produces captions from image features using a recurrent\n",
    "    neural network.\n",
    "\n",
    "    The RNN receives input vectors of size D, has a vocab size of V, works on\n",
    "    sequences of length T, has an RNN hidden dimension of H, uses word vectors\n",
    "    of dimension W, and operates on minibatches of size N.\n",
    "\n",
    "    Note that we don't use any regularization for the CaptioningRNN.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, word_to_idx, input_dim=512, wordvec_dim=128,\n",
    "                 hidden_dim=128, cell_type='rnn', dtype=np.float32):\n",
    "        \"\"\"\n",
    "        Construct a new CaptioningRNN instance.\n",
    "\n",
    "        Inputs:\n",
    "        - word_to_idx: A dictionary giving the vocabulary. It contains V entries,\n",
    "          and maps each string to a unique integer in the range [0, V).\n",
    "        - input_dim: Dimension D of input image feature vectors.　　### 图像的特征向量\n",
    "        - wordvec_dim: Dimension W of word vectors.　　　###词向量维度\n",
    "        - hidden_dim: Dimension H for the hidden state of the RNN.　　###隐藏层\n",
    "        - cell_type: What type of RNN to use; either 'rnn' or 'lstm'.　###神经元状态rnn or lstm\n",
    "        - dtype: numpy datatype to use; use float32 for training and float64 for\n",
    "          numeric gradient checking.\n",
    "        \"\"\"\n",
    "        if cell_type not in {'rnn', 'lstm'}:\n",
    "            raise ValueError('Invalid cell_type \"%s\"' % cell_type)\n",
    "\n",
    "        self.cell_type = cell_type\n",
    "        self.dtype = dtype\n",
    "        self.word_to_idx = word_to_idx\n",
    "        self.idx_to_word = {i: w for w, i in word_to_idx.items()}  ###idx_to_word\n",
    "        self.params = {}\n",
    "\n",
    "        vocab_size = len(word_to_idx)  ## 词典大小\n",
    "\n",
    "        ### ３个特殊字符的索引\n",
    "        self._null = word_to_idx['<NULL>']\n",
    "        self._start = word_to_idx.get('<START>', None)  # 用None表示START和END\n",
    "        self._end = word_to_idx.get('<END>', None)\n",
    "\n",
    "        # Initialize word vectors　初始化词向量\n",
    "        self.params['W_embed'] = np.random.randn(vocab_size, wordvec_dim)  ## (V, W)\n",
    "        self.params['W_embed'] /= 100  ##???\n",
    "\n",
    "        # Initialize CNN -> hidden state projection parameters  根据图像生成的特征向量到第一个隐藏层的矩阵初始化\n",
    "        self.params['W_proj'] = np.random.randn(input_dim, hidden_dim)\n",
    "        self.params['W_proj'] /= np.sqrt(input_dim)  ### 使得生成的初始权重的方差为0.2?\n",
    "        self.params['b_proj'] = np.zeros(hidden_dim)\n",
    "\n",
    "        # Initialize parameters for the RNN　　初始化RNN参数\n",
    "        dim_mul = {'lstm': 4, 'rnn': 1}[cell_type]  \n",
    "        self.params['Wx'] = np.random.randn(wordvec_dim, dim_mul * hidden_dim)\n",
    "        self.params['Wx'] /= np.sqrt(wordvec_dim)\n",
    "        self.params['Wh'] = np.random.randn(hidden_dim, dim_mul * hidden_dim)\n",
    "        self.params['Wh'] /= np.sqrt(hidden_dim)\n",
    "        self.params['b'] = np.zeros(dim_mul * hidden_dim)\n",
    "\n",
    "        # Initialize output to vocab weights 从隐藏层到输出词之间的矩阵初始化\n",
    "        self.params['W_vocab'] = np.random.randn(hidden_dim, vocab_size)  # (H,V)\n",
    "        self.params['W_vocab'] /= np.sqrt(hidden_dim)\n",
    "        self.params['b_vocab'] = np.zeros(vocab_size)\n",
    "\n",
    "        # Cast parameters to correct dtype\n",
    "        for k, v in self.params.items():\n",
    "            self.params[k] = v.astype(self.dtype)\n",
    "\n",
    "    def loss(self, features, captions):\n",
    "        \"\"\"\n",
    "        Compute training-time loss for the RNN. We input image features and\n",
    "        ground-truth captions for those images, and use an RNN (or LSTM) to compute\n",
    "        loss and gradients on all parameters.\n",
    "\n",
    "        Inputs:\n",
    "        - features: Input image features, of shape (N, D)　## 输入是图像的特征向量(N, D)\n",
    "        - captions: Ground-truth captions; an integer array of shape (N, T) where\n",
    "          each element is in the range 0 <= y[i, t] < V  ## 真实标签是标题对应在字典中的索引(N, T)\n",
    "\n",
    "        Returns a tuple of:\n",
    "        - loss: Scalar loss\n",
    "        - grads: Dictionary of gradients parallel to self.params\n",
    "        \"\"\"\n",
    "        # Cut captions into two pieces: captions_in has everything but the last word\n",
    "        # and will be input to the RNN; captions_out has everything but the first\n",
    "        # word and this is what we will expect the RNN to generate. These are offset\n",
    "        # by one relative to each other because the RNN should produce word (t+1)\n",
    "        # after receiving word t. The first element of captions_in will be the START\n",
    "        # token, and the first element of captions_out will be the first word.\n",
    "        \n",
    "        ## 这里将captions分成了两个部分，captions_in是除了最后一个词外的所有词，是输入到RNN/LSTM的输入；\n",
    "        ## captions_out是除了第一个词外的所有词，是RNN/LSTM期望得到的输出。\n",
    "        captions_in = captions[:, :-1]  #(N,T)而不是T-1，T是自己定的嘛，不是一定就得和整个句子长度一致，输入和输出都是T就没问题\n",
    "        captions_out = captions[:, 1:]  #真实值\n",
    "\n",
    "        # You'll need this\n",
    "        mask = (captions_out != self._null)\n",
    "\n",
    "        # Weight and bias for the affine transform from image features to initial\n",
    "        # hidden state\n",
    "        # 从图像特征到初始隐藏状态的权值矩阵和偏差值 \n",
    "        W_proj, b_proj = self.params['W_proj'], self.params['b_proj']\n",
    "\n",
    "        # Word embedding matrix\n",
    "        # 词嵌入矩阵\n",
    "        W_embed = self.params['W_embed']\n",
    "\n",
    "        # Input-to-hidden, hidden-to-hidden, and biases for the RNN\n",
    "        # RNN/LSTM参数\n",
    "        Wx, Wh, b = self.params['Wx'], self.params['Wh'], self.params['b']\n",
    "\n",
    "        # Weight and bias for the hidden-to-vocab transformation.\n",
    "        # 每一隐藏层到输出的权值矩阵和偏差\n",
    "        W_vocab, b_vocab = self.params['W_vocab'], self.params['b_vocab']\n",
    "\n",
    "        loss, grads = 0.0, {}\n",
    "        ############################################################################\n",
    "        # TODO: Implement the forward and backward passes for the CaptioningRNN.   #\n",
    "        # In the forward pass you will need to do the following:                   #\n",
    "        # (1) Use an affine transformation to compute the initial hidden state     #\n",
    "        #     from the image features. This should produce an array of shape (N, H)#\n",
    "        # (2) Use a word embedding layer to transform the words in captions_in     #\n",
    "        #     from indices to vectors, giving an array of shape (N, T, W).         #\n",
    "        # (3) Use either a vanilla RNN or LSTM (depending on self.cell_type) to    #\n",
    "        #     process the sequence of input word vectors and produce hidden state  #\n",
    "        #     vectors for all timesteps, producing an array of shape (N, T, H).    #\n",
    "        # (4) Use a (temporal) affine transformation to compute scores over the    #\n",
    "        #     vocabulary at every timestep using the hidden states, giving an      #\n",
    "        #     array of shape (N, T, V).                                            #\n",
    "        # (5) Use (temporal) softmax to compute loss using captions_out, ignoring  #\n",
    "        #     the points where the output word is <NULL> using the mask above.     #\n",
    "        #                                                                          #\n",
    "        # In the backward pass you will need to compute the gradient of the loss   #\n",
    "        # with respect to all model parameters. Use the loss and grads variables   #\n",
    "        # defined above to store loss and gradients; grads[k] should give the      #\n",
    "        # gradients for self.params[k].                                            #\n",
    "        ############################################################################\n",
    "        \n",
    "        ## step1: 图像特征到隐藏层：全连接层\n",
    "        h0 = np.dot(features, W_proj) + b_proj  # (N,H)\n",
    "        \n",
    "        ## step2: 将输入序列转换为词向量\n",
    "        embed_out, cache_embed = word_embedding_forward(captions_in, W_embed)  # (N ,T, W)\n",
    "        \n",
    "        ## step3: 将图像特征和词向量作为输入，通过RNN,得到隐藏层的输出\n",
    "        if self.cell_type == 'rnn':\n",
    "            hidden_out, cache_hidden = rnn_forward(embed_out, h0, Wx, Wh, b) \n",
    "        elif self.cell_type == 'lstm':\n",
    "            hidden_out, cache_hidden = lstm_forward(embed_out, h0, Wx, Wh, b)\n",
    "            assert(len(cache_hidden[0])==15)\n",
    "        else:\n",
    "            raise ValueError('%s not implemented' % (self.cell_type))\n",
    "        \n",
    "        ## step4: 将隐藏层的输出作为输入，通过affine-softmax层得到输出，词典中每个词的得分\n",
    "        score_vocab, cache_vocab = temporal_affine_forward(hidden_out, W_vocab, b_vocab)  # (N, T, V)\n",
    "        \n",
    "        # 用softmax函数计算损失，真实值为captions_out, 用mask忽视所有向量中<NULL>词汇\n",
    "        loss, dscore_vocab = temporal_softmax_loss(score_vocab, captions_out, mask, verbose=False)\n",
    "\n",
    "        ## 反向传播计算梯度\n",
    "        grads = dict.fromkeys(self.params)\n",
    "        \n",
    "        # step4 backward\n",
    "        dhidden_out, dW_vocab, db_vocab = temporal_affine_backward(dscore_vocab, cache_vocab)\n",
    "        grads['W_vocab'] = dW_vocab\n",
    "        grads['b_vocab'] = db_vocab\n",
    "\n",
    "        # Backward into step 3\n",
    "        if self.cell_type == 'rnn':\n",
    "            dembed_out, dh0, dWx, dWh, db = rnn_backward(dhidden_out, cache_hidden)\n",
    "        elif self.cell_type == 'lstm':\n",
    "            assert(len(cache_hidden[0])==15)\n",
    "            dembed_out, dh0, dWx, dWh, db = lstm_backward(dhidden_out, cache_hidden)\n",
    "        else:\n",
    "            raise ValueError('%s not implemented' % (self.cell_type))\n",
    "        grads['Wx'] = dWx\n",
    "        grads['Wh'] = dWh\n",
    "        grads['b'] = db\n",
    "\n",
    "        # Backward into step 2\n",
    "        dW_embed = word_embedding_backward(dembed_out, cache_embed)\n",
    "        grads['W_embed'] = dW_embed\n",
    "        \n",
    "        ## aBackward into step 1\n",
    "        dW_proj = features.T.dot(dh0)\n",
    "        db_proj = np.sum(dh0,axis=0)\n",
    "        grads['W_proj'] = dW_proj\n",
    "        grads['b_proj'] = db_proj\n",
    "        ############################################################################\n",
    "        #                             END OF YOUR CODE                             #\n",
    "        ############################################################################\n",
    "\n",
    "        return loss, grads\n",
    "\n",
    "    def sample(self, features, max_length=30):\n",
    "        \"\"\"\n",
    "        Run a test-time forward pass for the model, sampling captions for input\n",
    "        feature vectors.\n",
    "\n",
    "        At each timestep, we embed the current word, pass it and the previous hidden\n",
    "        state to the RNN to get the next hidden state, use the hidden state to get\n",
    "        scores for all vocab words, and choose the word with the highest score as\n",
    "        the next word. The initial hidden state is computed by applying an affine\n",
    "        transform to the input image features, and the initial word is the <START>\n",
    "        token.\n",
    "\n",
    "        For LSTMs you will also have to keep track of the cell state; in that case\n",
    "        the initial cell state should be zero.\n",
    "\n",
    "        Inputs:\n",
    "        - features: Array of input image features of shape (N, D).\n",
    "        - max_length: Maximum length T of generated captions.\n",
    "\n",
    "        Returns:\n",
    "        - captions: Array of shape (N, max_length) giving sampled captions,\n",
    "          where each element is an integer in the range [0, V). The first element\n",
    "          of captions should be the first sampled word, not the <START> token.\n",
    "        \"\"\"\n",
    "        N = features.shape[0]\n",
    "        captions = self._null * np.ones((N, max_length), dtype=np.int32)\n",
    "\n",
    "        # Unpack parameters\n",
    "        W_proj, b_proj = self.params['W_proj'], self.params['b_proj']\n",
    "        W_embed = self.params['W_embed']\n",
    "        Wx, Wh, b = self.params['Wx'], self.params['Wh'], self.params['b']\n",
    "        W_vocab, b_vocab = self.params['W_vocab'], self.params['b_vocab']\n",
    "\n",
    "        ###########################################################################\n",
    "        # TODO: Implement test-time sampling for the model. You will need to      #\n",
    "        # initialize the hidden state of the RNN by applying the learned affine   #\n",
    "        # transform to the input image features. The first word that you feed to  #\n",
    "        # the RNN should be the <START> token; its value is stored in the         #\n",
    "        # variable self._start. At each timestep you will need to do to:          #\n",
    "        # (1) Embed the previous word using the learned word embeddings           #\n",
    "        # (2) Make an RNN step using the previous hidden state and the embedded   #\n",
    "        #     current word to get the next hidden state.                          #\n",
    "        # (3) Apply the learned affine transformation to the next hidden state to #\n",
    "        #     get scores for all words in the vocabulary                          #\n",
    "        # (4) Select the word with the highest score as the next word, writing it #\n",
    "        #     to the appropriate slot in the captions variable                    #\n",
    "        #                                                                         #\n",
    "        # For simplicity, you do not need to stop generating after an <END> token #\n",
    "        # is sampled, but you can if you want to.                                 #\n",
    "        #                                                                         #\n",
    "        # HINT: You will not be able to use the rnn_forward or lstm_forward       #\n",
    "        # functions; you'll need to call rnn_step_forward or lstm_step_forward in #\n",
    "        # a loop.                                                                 #\n",
    "        ###########################################################################\n",
    "        \n",
    "        ## step1: 图像特征到隐藏层：全连接层\n",
    "        N, D = features.shape\n",
    "        H, V = W_vocab.shape\n",
    "        V, W = W_embed.shape\n",
    "        \n",
    "        h0 = np.dot(features, W_proj) + b_proj  # (N,H)\n",
    "        assert(h0.shape==(N,H))\n",
    "        \n",
    "        captions[:,0] = self._start \n",
    "        \n",
    "        prev_h = h0 # Previous hidden state\n",
    "        prev_c = np.zeros(h0.shape)  # lstm: memory cell state\n",
    "        # Current word (start word)\n",
    "        x = self._start * np.ones((N, 1), dtype=np.int32)  # (N,1) 测试：sample时输入\n",
    "        \n",
    "        for t in range(max_length):\n",
    "            embed_out, _ = word_embedding_forward(x, W_embed)  # (N ,1, W) embedded word vector\n",
    "            assert(embed_out.shape == (N,1,W))\n",
    "            if self.cell_type == 'rnn':\n",
    "                # Run a step of rnn   # Remove single-dimensional entries from the shape of an array.\n",
    "                hidden_out, _ = rnn_step_forward(np.squeeze(embed_out), prev_h, Wx, Wh, b)   ## (N,H)                \n",
    "            elif self.cell_type == 'lstm':\n",
    "                # Run a step of lstm\n",
    "                hidden_out, _, _ = lstm_step_forward(np.squeeze(embed_out), prev_h, prev_c, Wx, Wh, b)\n",
    "            else:\n",
    "                raise ValueError('%s not implemented' % (self.cell_type))\n",
    "        \n",
    "        # Compute the score distrib over the dictionary      \n",
    "        score_vocab, cache_vocab = temporal_affine_forward(hidden_out[:, np.newaxis, :], W_vocab, b_vocab)  # (N, 1, V)\n",
    "        # Squeeze unecessari dimension and get the best word idx\n",
    "        idx_best = np.squeeze(np.argmax(score_vocab, axis=2))\n",
    "        # Put it in the captions\n",
    "        captions[:, t] = idx_best\n",
    "        \n",
    "        ############################################################################\n",
    "        #                             END OF YOUR CODE                             #\n",
    "        ############################################################################\n",
    "        return captions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ４.训练模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 优化算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(w, dw, config=None):\n",
    "    \"\"\"\n",
    "    Performs vanilla stochastic gradient descent.\n",
    "\n",
    "    config format:\n",
    "    - learning_rate: Scalar learning rate.\n",
    "    \"\"\"\n",
    "    if config is None: config = {}\n",
    "    config.setdefault('learning_rate', 1e-2)\n",
    "\n",
    "    w -= config['learning_rate'] * dw\n",
    "    return w, config\n",
    "\n",
    "\n",
    "def adam(x, dx, config=None):\n",
    "    \"\"\"\n",
    "    Uses the Adam update rule, which incorporates moving averages of both the\n",
    "    gradient and its square and a bias correction term.\n",
    "\n",
    "    config format:\n",
    "    - learning_rate: Scalar learning rate.\n",
    "    - beta1: Decay rate for moving average of first moment of gradient.\n",
    "    - beta2: Decay rate for moving average of second moment of gradient.\n",
    "    - epsilon: Small scalar used for smoothing to avoid dividing by zero.\n",
    "    - m: Moving average of gradient.\n",
    "    - v: Moving average of squared gradient.\n",
    "    - t: Iteration number.\n",
    "    \"\"\"\n",
    "    if config is None: config = {}\n",
    "    config.setdefault('learning_rate', 1e-3)\n",
    "    config.setdefault('beta1', 0.9)\n",
    "    config.setdefault('beta2', 0.999)\n",
    "    config.setdefault('epsilon', 1e-8)\n",
    "    config.setdefault('m', np.zeros_like(x))\n",
    "    config.setdefault('v', np.zeros_like(x))\n",
    "    config.setdefault('t', 0)\n",
    "\n",
    "    next_x = None\n",
    "    beta1, beta2, eps = config['beta1'], config['beta2'], config['epsilon']\n",
    "    t, m, v = config['t'], config['m'], config['v']\n",
    "    m = beta1 * m + (1 - beta1) * dx\n",
    "    v = beta2 * v + (1 - beta2) * (dx * dx)\n",
    "    t += 1\n",
    "    alpha = config['learning_rate'] * np.sqrt(1 - beta2 ** t) / (1 - beta1 ** t)\n",
    "    x -= alpha * (m / (np.sqrt(v) + eps))\n",
    "    config['t'] = t\n",
    "    config['m'] = m\n",
    "    config['v'] = v\n",
    "    next_x = x\n",
    "\n",
    "    return next_x, config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 小批量梯度下降"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_coco_minibatch(data, batch_size=100, split='train'):\n",
    "    split_size = data['%s_captions' % split].shape[0]\n",
    "    mask = np.random.choice(split_size, batch_size)\n",
    "    captions = data['%s_captions' % split][mask]\n",
    "    image_idxs = data['%s_image_idxs' % split][mask]\n",
    "    image_features = data['%s_features' % split][image_idxs]\n",
    "    urls = data['%s_urls' % split][image_idxs]\n",
    "    return captions, image_features, urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cs231n import optim\n",
    "class CaptioningSolver(object):\n",
    "    \"\"\"\n",
    "    A CaptioningSolver encapsulates all the logic necessary for training\n",
    "    image captioning models. The CaptioningSolver performs stochastic gradient\n",
    "    descent using different update rules defined in optim.py.\n",
    "\n",
    "    The solver accepts both training and validataion data and labels so it can\n",
    "    periodically check classification accuracy on both training and validation\n",
    "    data to watch out for overfitting.\n",
    "\n",
    "    To train a model, you will first construct a CaptioningSolver instance,\n",
    "    passing the model, dataset, and various options (learning rate, batch size,\n",
    "    etc) to the constructor. You will then call the train() method to run the\n",
    "    optimization procedure and train the model.\n",
    "\n",
    "    After the train() method returns, model.params will contain the parameters\n",
    "    that performed best on the validation set over the course of training.\n",
    "    In addition, the instance variable solver.loss_history will contain a list\n",
    "    of all losses encountered during training and the instance variables\n",
    "    solver.train_acc_history and solver.val_acc_history will be lists containing\n",
    "    the accuracies of the model on the training and validation set at each epoch.\n",
    "\n",
    "    Example usage might look something like this:\n",
    "\n",
    "    data = load_coco_data()\n",
    "    model = MyAwesomeModel(hidden_dim=100)\n",
    "    solver = CaptioningSolver(model, data,\n",
    "                    update_rule='sgd',\n",
    "                    optim_config={\n",
    "                      'learning_rate': 1e-3,\n",
    "                    },\n",
    "                    lr_decay=0.95,\n",
    "                    num_epochs=10, batch_size=100,\n",
    "                    print_every=100)\n",
    "    solver.train()\n",
    "\n",
    "\n",
    "    A CaptioningSolver works on a model object that must conform to the following\n",
    "    API:\n",
    "\n",
    "    - model.params must be a dictionary mapping string parameter names to numpy\n",
    "      arrays containing parameter values.\n",
    "\n",
    "    - model.loss(features, captions) must be a function that computes\n",
    "      training-time loss and gradients, with the following inputs and outputs:\n",
    "\n",
    "      Inputs:\n",
    "      - features: Array giving a minibatch of features for images, of shape (N, D\n",
    "      - captions: Array of captions for those images, of shape (N, T) where\n",
    "        each element is in the range (0, V].\n",
    "\n",
    "      Returns:\n",
    "      - loss: Scalar giving the loss\n",
    "      - grads: Dictionary with the same keys as self.params mapping parameter\n",
    "        names to gradients of the loss with respect to those parameters.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, data, **kwargs):\n",
    "        \"\"\"\n",
    "        Construct a new CaptioningSolver instance.\n",
    "\n",
    "        Required arguments:\n",
    "        - model: A model object conforming to the API described above\n",
    "        - data: A dictionary of training and validation data from load_coco_data\n",
    "\n",
    "        Optional arguments:\n",
    "        - update_rule: A string giving the name of an update rule in optim.py.\n",
    "          Default is 'sgd'.\n",
    "        - optim_config: A dictionary containing hyperparameters that will be\n",
    "          passed to the chosen update rule. Each update rule requires different\n",
    "          hyperparameters (see optim.py) but all update rules require a\n",
    "          'learning_rate' parameter so that should always be present.\n",
    "        - lr_decay: A scalar for learning rate decay; after each epoch the learning\n",
    "          rate is multiplied by this value.\n",
    "        - batch_size: Size of minibatches used to compute loss and gradient during\n",
    "          training.\n",
    "        - num_epochs: The number of epochs to run for during training.\n",
    "        - print_every: Integer; training losses will be printed every print_every\n",
    "          iterations.\n",
    "        - verbose: Boolean; if set to false then no output will be printed during\n",
    "          training.\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.data = data\n",
    "\n",
    "        # Unpack keyword arguments                            ### take a note!!!!\n",
    "        self.update_rule = kwargs.pop('update_rule', 'sgd')   ### kwargs是一个参数字典，当里面没有'update_rule'时，返回'sgd'\n",
    "        self.optim_config = kwargs.pop('optim_config', {})    ###　如果给出了'update_rule'\n",
    "        self.lr_decay = kwargs.pop('lr_decay', 1.0)\n",
    "        self.batch_size = kwargs.pop('batch_size', 100)\n",
    "        self.num_epochs = kwargs.pop('num_epochs', 10)\n",
    "\n",
    "        self.print_every = kwargs.pop('print_every', 10)\n",
    "        self.verbose = kwargs.pop('verbose', True)\n",
    "\n",
    "        # Throw an error if there are extra keyword arguments\n",
    "        if len(kwargs) > 0:\n",
    "            extra = ', '.join('\"%s\"' % k for k in list(kwargs.keys()))\n",
    "            raise ValueError('Unrecognized arguments %s' % extra)\n",
    "\n",
    "        # Make sure the update rule exists, then replace the string\n",
    "        # name with the actual function\n",
    "        if not hasattr(optim, self.update_rule):  #判断是否有这个优化算法\n",
    "            raise ValueError('Invalid update_rule \"%s\"' % self.update_rule)\n",
    "        self.update_rule = getattr(optim, self.update_rule)\n",
    "\n",
    "        self._reset()\n",
    "\n",
    "\n",
    "    def _reset(self):\n",
    "        \"\"\"\n",
    "        Set up some book-keeping variables for optimization. Don't call this\n",
    "        manually.   ### 需要进行优化的变量，不要手动调用\n",
    "        \"\"\"\n",
    "        # Set up some variables for book-keeping\n",
    "        self.epoch = 0\n",
    "        self.best_val_acc = 0\n",
    "        self.best_params = {}\n",
    "        self.loss_history = []\n",
    "        self.train_acc_history = []\n",
    "        self.val_acc_history = []\n",
    "\n",
    "        # Make a deep copy of the optim_config for each parameter\n",
    "        self.optim_configs = {}\n",
    "        for p in self.model.params:\n",
    "            d = {k: v for k, v in self.optim_config.items()}\n",
    "            self.optim_configs[p] = d\n",
    "\n",
    "\n",
    "    def _step(self):   ### 被调用一次就执行一步梯度下降，在train()中倍调用，不要手动调用\n",
    "        \"\"\"\n",
    "        Make a single gradient update. This is called by train() and should not\n",
    "        be called manually.\n",
    "        \"\"\"\n",
    "        # Make a minibatch of training data\n",
    "        minibatch = sample_coco_minibatch(self.data,\n",
    "                      batch_size=self.batch_size,\n",
    "                      split='train')\n",
    "        captions, features, urls = minibatch\n",
    "\n",
    "        # Compute loss and gradient\n",
    "        loss, grads = self.model.loss(features, captions)\n",
    "        self.loss_history.append(loss)\n",
    "\n",
    "        # Perform a parameter update\n",
    "        for p, w in self.model.params.items():\n",
    "            dw = grads[p]\n",
    "            config = self.optim_configs[p]\n",
    "            next_w, next_config = self.update_rule(w, dw, config)  ###　梯度下降，在optim.py里面\n",
    "            self.model.params[p] = next_w\n",
    "            self.optim_configs[p] = next_config\n",
    "\n",
    "\n",
    "    # TODO: This does nothing right now; maybe implement BLEU?\n",
    "    def check_accuracy(self, X, y, num_samples=None, batch_size=100):\n",
    "        \"\"\"\n",
    "        Check accuracy of the model on the provided data.\n",
    "\n",
    "        Inputs:\n",
    "        - X: Array of data, of shape (N, d_1, ..., d_k)\n",
    "        - y: Array of labels, of shape (N,)\n",
    "        - num_samples: If not None, subsample the data and only test the model\n",
    "          on num_samples datapoints.\n",
    "        - batch_size: Split X and y into batches of this size to avoid using too\n",
    "          much memory.\n",
    "\n",
    "        Returns:\n",
    "        - acc: Scalar giving the fraction of instances that were correctly\n",
    "          classified by the model.\n",
    "        \"\"\"\n",
    "        return 0.0\n",
    "\n",
    "        # Maybe subsample the data\n",
    "        N = X.shape[0]\n",
    "        if num_samples is not None and N > num_samples:\n",
    "            mask = np.random.choice(N, num_samples)\n",
    "            N = num_samples\n",
    "            X = X[mask]\n",
    "            y = y[mask]\n",
    "\n",
    "        # Compute predictions in batches\n",
    "        num_batches = N / batch_size\n",
    "        if N % batch_size != 0:\n",
    "            num_batches += 1\n",
    "        y_pred = []\n",
    "        for i in range(num_batches):\n",
    "            start = i * batch_size\n",
    "            end = (i + 1) * batch_size\n",
    "            scores = self.model.loss(X[start:end])\n",
    "            y_pred.append(np.argmax(scores, axis=1))\n",
    "        y_pred = np.hstack(y_pred)\n",
    "        acc = np.mean(y_pred == y)\n",
    "\n",
    "        return acc\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Run optimization to train the model.\n",
    "        \"\"\"\n",
    "        num_train = self.data['train_captions'].shape[0]\n",
    "        iterations_per_epoch = max(num_train // self.batch_size, 1)\n",
    "        num_iterations = self.num_epochs * iterations_per_epoch     #总迭代数\n",
    "\n",
    "        for t in range(num_iterations):\n",
    "            self._step() ### 每一个batch更新一次梯度\n",
    "\n",
    "            # Maybe print training loss\n",
    "            if self.verbose and t % self.print_every == 0:\n",
    "                print('(Iteration %d / %d) loss: %f' % (\n",
    "                       t + 1, num_iterations, self.loss_history[-1]))\n",
    "\n",
    "            # At the end of every epoch, increment the epoch counter and decay the\n",
    "            # learning rate.\n",
    "            epoch_end = (t + 1) % iterations_per_epoch == 0\n",
    "            if epoch_end:\n",
    "                self.epoch += 1\n",
    "                for k in self.optim_configs:\n",
    "                    self.optim_configs[k]['learning_rate'] *= self.lr_decay ###　每一个epoch更新一次学习率\n",
    "\n",
    "            # Check train and val accuracy on the first iteration, the last\n",
    "            # iteration, and at the end of each epoch.\n",
    "            # TODO: Implement some logic to check Bleu on validation set periodically\n",
    "\n",
    "        # At the end of training swap the best params into the model\n",
    "        # self.model.params = self.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 1 / 100) loss: 79.551150\n",
      "(Iteration 11 / 100) loss: 43.829101\n",
      "(Iteration 21 / 100) loss: 30.062620\n",
      "(Iteration 31 / 100) loss: 14.020130\n",
      "(Iteration 41 / 100) loss: 6.004623\n",
      "(Iteration 51 / 100) loss: 1.847776\n",
      "(Iteration 61 / 100) loss: 0.639553\n",
      "(Iteration 71 / 100) loss: 0.280834\n",
      "(Iteration 81 / 100) loss: 0.234212\n",
      "(Iteration 91 / 100) loss: 0.123417\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa10ee1d470>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.random.seed(231)\n",
    "\n",
    "small_data = load_coco_data(max_train=50)\n",
    "\n",
    "small_lstm_model = CaptioningRNN(\n",
    "          cell_type='lstm',\n",
    "          word_to_idx=data['word_to_idx'],\n",
    "          input_dim=data['train_features'].shape[1],\n",
    "          hidden_dim=512,\n",
    "          wordvec_dim=256,\n",
    "          dtype=np.float32,\n",
    "        )\n",
    "\n",
    "small_lstm_solver = CaptioningSolver(small_lstm_model, small_data,\n",
    "           update_rule='adam',\n",
    "           num_epochs=50,\n",
    "           batch_size=25,\n",
    "           optim_config={\n",
    "             'learning_rate': 5e-3,\n",
    "           },\n",
    "           lr_decay=0.995,\n",
    "           verbose=True, print_every=10,\n",
    "         )\n",
    "\n",
    "small_lstm_solver.train()\n",
    "\n",
    "# Plot the training losses\n",
    "plt.plot(small_lstm_solver.loss_history)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training loss history')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
