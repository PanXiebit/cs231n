{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 词表示word representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 one-hot representation\n",
    "任何两个词的内积都是０，即任何两个向量的距离毫无价值。\n",
    "\n",
    "#### 1.2 特征表示方法(Featurized representation): 词嵌入(word embedding)\n",
    "\n",
    "Visualizing word embedding　词嵌入可视化t-sne\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180301153442438?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcGFueGlhb3hpZQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "transfer learning and word embeddings\n",
    "- learning word embedding from large text corpus.(1-100B words)\n",
    "  or download per-trained embedding online\n",
    "- transfer embedding to new task with smaller training set. (say, 100k words)\n",
    "- Optional: continue to finetune the word embedding with new data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 properities of word embedding :analogy reasoning 类比推理\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180301155848557?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcGFueGlhb3hpZQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过词向量之间的距离，进行类比推理。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cosine similarity　表征词向量之间的距离"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$sim(e_{w}, e_{king}-e_{man}+e{women})$\n",
    "\n",
    "$sim(u,v) = \\dfrac{u^Tv}{||u||_2||v||_2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 Embedding matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding matrix: 300x100000  通过学习得到～\n",
    "\n",
    "one-hot vector: 100000x1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5 learning word embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Neural language model:** a reasonable way to learn a set of embedding\n",
    "\n",
    "[Bengio et, al. 2003, Aneural probabilistic language model]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![这里写图片描述](http://img.blog.csdn.net/20180301165610357?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcGFueGlhb3hpZQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n",
    "\n",
    "通过训练语言模型来学习Embedding,在图中神经网络参数有E, $W^{[1]},b^{[1]},W^{[2]},b^{[2]}$\n",
    "\n",
    "why?\n",
    "- 假设有两个训练sequence\n",
    "I want a glass of orange (juice).\n",
    "I want a glass of apple (juice).\n",
    "神经网络预测结果与对应真实标签juice越接近越好，显然在Embedding中apple和orange的向量表示也是越接近越好～"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练Embedding与训练语言模型不同,起所需的上下文可以有各种形式～\n",
    "\n",
    "other context/target pairs\n",
    "\n",
    "Context:Last 4 words.\n",
    "\n",
    "- 4 words on left&right\n",
    "- Last 1 word\n",
    "- Nearby 1 word   ----- Skip-Gram model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Mikolov et.al.,2013. Eficient estimation of word representations in vector space.]\n",
    "\n",
    "#### 2.1 Skip-Grams\n",
    "\n",
    "I want a glass of orange juice to go along with my cereal.\n",
    "\n",
    "$$\n",
    "\\begin{array}{clcr}\n",
    "\\text{Content} & \\text{Target}\\\\\n",
    "\\hline\n",
    "orange & juice \\\\\n",
    "orange & glass \\\\\n",
    "orange & my\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "随机选择一个词作为上下文Ｃontent，比如orange,随机在一定词距内(10个或５个..)选另一个词作为目标词Target.\n",
    "\n",
    "构造这个监督模型并不是为了解决模型本身，而是通过学习这个模型来学到一个好的词嵌入模型word embedding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Model\n",
    "\n",
    "content c (\"orange\") -------> Target t (\"juice\")\n",
    "\n",
    "$O_c(one-hot vector)--> E (embedding matrix)--> e_c --> softmax --> \\hat y$\n",
    "\n",
    "softmax: $p(t|c) = \\dfrac{e^{\\theta_t^Te_c}}{\\sum_{j=1}^{100000}e^{\\theta_j^Te_c}}\\quad$  $\\theta_t是关于输出矩阵的参数$\n",
    "\n",
    "$L(\\hat y,y) = -\\sum_{i=1}^{100000}y_ilog\\hat y_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problems with softmax classification\n",
    "\n",
    "$p(t|c) = \\dfrac{e^{\\theta_t^Te_c}}{\\sum_{j=1}^{100000}e^{\\theta_j^Te_c}}\\quad$ 计算量太太大\n",
    "\n",
    "hierarchical softmax classifier　分级softmax分类器\n",
    "\n",
    "#### How to sample the context c?\n",
    "实际上词p(c)的分布不是单纯的在训练语料库上均匀且随机的采样得到的，而是采用了不同的启发式different heuristics来平衡更常见的词。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CBOW Continue Bag-Of-Words Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ３．负采样(Negative sampling)\n",
    "[Mikolov et.al.,2013. Distributed representation of words and phrases and their compositionality]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 定义一个新的学习模型\n",
    "\n",
    "I want a glass of orange juice to go along with my cereal.\n",
    "\n",
    "$$\n",
    "\\begin{array}{clcr}\n",
    "\\text{Content} & \\text{Word}　& \\text{Target?}\\\\\n",
    "\\hline\n",
    "orange & juice & 1 \\\\\n",
    "orange & glass & 0 \\\\\n",
    "orange & king & 0 \\\\\n",
    "orange & book & 0 \\\\\n",
    "orange & the & 0 \\\\\n",
    "orange & of & 0\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "学习模型：\n",
    "- 输入x: c(context), t(word)\n",
    "- 输出y: y(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "负采样个数k:\n",
    "\n",
    "k = 5-20  smaller datasets\n",
    "\n",
    "k = 2-5 larger datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "softmax方法:   $p(t|c) = \\dfrac{e^{\\theta_t^Te_c}}{\\sum_{j=1}^{100000}e^{\\theta_j^Te_c}}\\quad$  \n",
    "\n",
    "负采样：$P(y=1|c,t) = \\sigma(\\theta_t^Te_c)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "随机选择的上下文context: orange　　目标词target word: juice\n",
    "\n",
    "\n",
    "$O_{6257}$-->E--> $e_{6257}$\n",
    "\n",
    "one-hot vector--> Embedding matrix--> embedding vector \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![这里写图片描述](http://img.blog.csdn.net/20180302101956628?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcGFueGlhb3hpZQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**k+1个logistic分类器：**\n",
    "\n",
    "嵌入向量$e_c$乘以输出矩阵$\\theta_t$，再作用于sigmoid函数，得到$\\hat y = \\sigma(\\theta_t^Te_c)$\n",
    "\n",
    "logistic损失函数$L(\\hat y,y) = -ylog\\hat y-(1-y)log(1-\\hat y)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 如何选择负采样样本\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180302103931605?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcGFueGlhb3hpZQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n",
    "\n",
    "对于负样本的选择，有两个极端，一个是通过经验频率(emprical frequency)采样，一个是根据词汇表均匀且随机的采样。\n",
    "\n",
    "Mikolov采用的方法介于两者之间：\n",
    "$P(w_i) = \\dfrac{f(w_i)^{3/4}}{\\sum_{j=1}^{10000}f(w_j)^{3/4}}$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
