{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.为什么选择序列模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. speech recognition\n",
    "2. music generation\n",
    "3. sentiment classification\n",
    "4. DNA sequence analysis\n",
    "5. machine translation\n",
    "6. vedio activity recognition\n",
    "7. name entity recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 2. 数学符号 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![这里写图片描述](http://img.blog.csdn.net/20180228100726241?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcGFueGlhb3hpZQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$x^{(i)<t>} 表示第i个样本输入的第t个元素$,$T_x^{(i)}表示元素个数$\n",
    "\n",
    "$\\hat y^{(i)<t>} 表示第i个样本标签的第t个元素$,$T_y^{(i)}表示元素个数$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 循环神经网络模型\n",
    "\n",
    "3.1 why not a standard network?\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180228101623772?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcGFueGlhb3hpZQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n",
    "\n",
    "有两个问题:\n",
    "1. 输入和输出在不同的样本中可能是不同的长度\n",
    "2. Don not share features learned across different positions of text. \n",
    " 在文本的不同位置学习到的特征不能共享."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2 Recurrent Naural networks\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180228104142572?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcGFueGlhb3hpZQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forward propagation\n",
    "\n",
    "$a^{<t>} = g(W_{aa}a{<t-1>} + W_{ax}x{<t>} + b_a)$\n",
    "\n",
    "$\\hat y^{<t>} = g(W_{ya}a{<t>} + b_y)$\n",
    "\n",
    "也可以写成:\n",
    "\n",
    "$a^{<t>} = g(W_{a}[a{<t-1>},x{<t>}] + b_a)$, \n",
    "\n",
    "$\\hat y^{<t>} = g(W_{y}a{<t>} + b_y)$\n",
    "\n",
    "其中:\n",
    "\n",
    "$[W_{aa},W_{ax}] = W_a$\n",
    "\n",
    "$\n",
    "    [a^{<t-1>},x^{<t>}] = \n",
    "    \\begin{bmatrix}\n",
    "    a^{<t-1>} \\\\\n",
    "    x^{<t>} \\\\\n",
    "    \\end{bmatrix}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 通过时间的反向传播 backpropagation through time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![这里写图片描述](http://img.blog.csdn.net/20180228110437156?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcGFueGlhb3hpZQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n",
    "\n",
    "这个图画的棒棒的~~\n",
    "\n",
    "再回想下:是否解决了之前的传统神经网络的问题呢?\n",
    "\n",
    "权重参数是共享的,解决了参数过多的问题."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 不同类型的循环神经网络 different type of RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输入与输出长度不同 $T_x != T_y$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Andrej Karpathy \"Unreasonable Effectiveness of Recurrent Nerual Networks\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![这里写图片描述](http://img.blog.csdn.net/20180228114823669?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcGFueGlhb3hpZQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "one to many : generation music\n",
    "\n",
    "many to one : sentimant analysis\n",
    "\n",
    "many to many : name entity recognition\n",
    "\n",
    "many to many : machine translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. 语言模型和序列生成 language model and sequence generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如何训练语言模型:\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180228122243305?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcGFueGlhb3hpZQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$x^{<t>} = y^{<t-1>}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. 对新序列采样\n",
    "\n",
    "了解语言模型学到了什么?需要对新序列进行采样. have a sample novel sequence\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180228124649353?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcGFueGlhb3hpZQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$y^{<t-1>} = np.random.choice()$\n",
    "\n",
    "$x^{<t>} = y^{<t-1>}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. 梯度消失和梯度爆炸"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "１．解决梯度爆炸，使用gradient clipping即能很好的解决\n",
    "２．梯度消失相对更困难。梯度消失不仅会造成神经网络难以训练，而且在语言模型中，之后的向量损失很难对之前较远的向量造成影响。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
