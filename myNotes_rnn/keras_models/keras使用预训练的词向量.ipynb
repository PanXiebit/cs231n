{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "原文连接：[Using pre-trained word embeddings in a Keras model](https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html)\n",
    "\n",
    "中文文档：[在Keras模型中使用预训练的词向量](http://keras-cn-docs.readthedocs.io/zh_CN/latest/blog/word_embedding/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本文将讲述使用预训练的词向量和卷积神经网络来实现文本分类问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实验方法\n",
    "以下是我们如何解决分类问题的步骤\n",
    "\n",
    "- 将所有的新闻样本转化为词索引序列。所谓词索引就是为每一个词依次分配一个整数ID。遍历所有的新闻文本，我们只保留最参见的20,000个词，而且 每个新闻文本最多保留1000个词。\n",
    "- 生成一个词向量矩阵。第i列表示词索引为i的词的词向量。\n",
    "- 将词向量矩阵载入Keras Embedding层，设置该层的权重不可再训练（也就是说在之后的网络训练过程中，词向量不再改变）。\n",
    "- Keras Embedding层之后连接一个1D的卷积层，并用一个softmax全连接输出新闻类别"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据预处理\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "查看训练样本文件以及内容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################\n",
      "Xref: cantaloupe.srv.cs.cmu.edu alt.atheism:49960 alt.atheism.moderated:713 news.answers:7054 alt.answers:126\n",
      "Path: cantaloupe.srv.cs.cmu.edu!crabapple.srv.cs.cmu.edu!bb3.andrew.cmu.edu!news.sei.cmu.edu!cis.ohio-state.edu!magnus.acs.ohio-state.edu!usenet.ins.cwru.edu!agate!spool.mu.edu!uunet!pipex!ibmpcug!mantis!mathew\n",
      "From: mathew <mathew@mantis.co.uk>\n",
      "Newsgroups: alt.atheism,alt.atheism.moderated,news.answers,alt.answers\n",
      "Subject: Alt.Atheism FAQ: Atheist Resources\n",
      "Summary: Books, addresses, music -- anything related to atheism\n",
      "Keywords: FAQ, atheism, books, music, fiction, addresses, contacts\n",
      "Message-ID: <19930329115719@mantis.co.uk>\n",
      "Date: Mon, 29 Mar 1993 11:57:19 GMT\n",
      "Expires: Thu, 29 Apr 1993 11:57:19 GMT\n",
      "Followup-To: alt.atheism\n",
      "Distribution: world\n",
      "Organization: Mantis Consultants, Cambridge. UK.\n",
      "Approved: news-answers-request@mit.edu\n",
      "Supersedes: <19930301143317@mantis.co.uk>\n",
      "Lines: 290\n",
      "\n",
      "Archive-name: atheism/resources\n",
      "Alt-atheism-archive-name: resources\n",
      "Last-modified: 11 December\n",
      "################\n",
      "904\n",
      "\n",
      "\n",
      "Archive-name: atheism/resources\n",
      "Alt-atheism-archive-name: resources\n",
      "Last-modified: 11 December 199\n"
     ]
    }
   ],
   "source": [
    "fpath1 = '/home/panxie/Documents/cs231n/myNotes_rnn/keras_models/20_newsgroup/alt.atheism/49960'\n",
    "with open(fpath1, encoding='latin-1') as f:\n",
    "    t = f.read()\n",
    "    print(\"################\")\n",
    "    print(t[:1000])\n",
    "    i = t.find('\\n\\n')\n",
    "    print(\"################\")\n",
    "    print(i)  ##904 意味着前面905个字符删掉，删除头部信息，保留新闻信息\n",
    "    if i > 0:\n",
    "        t = t[i:]\n",
    "    print(t[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "遍历训练数据下的文件夹，并获得不同类别的新闻以及对应的类别标签.\n",
    "每一个文件即新闻作为一个sequence，并存放在texts列表中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 19997 texts.\n"
     ]
    }
   ],
   "source": [
    "### 遍历训练数据下的文件夹，并获得不同类别的新闻以及对应的类别标签，\n",
    "texts = []\n",
    "labels_index = {}\n",
    "labels = []\n",
    "TEXT_DATA_DIR = '/home/panxie/Documents/cs231n/myNotes_rnn/keras_models/20_newsgroup'\n",
    "for name in sorted(os.listdir(TEXT_DATA_DIR)):\n",
    "    path = os.path.join(TEXT_DATA_DIR, name)\n",
    "    if os.path.isdir(path):\n",
    "        label_id = len(labels_index) ## 类别标签从０开始加１\n",
    "        labels_index[name] = label_id\n",
    "        for fname in sorted(os.listdir(path)):\n",
    "            if fname.isdigit():\n",
    "                fpath = os.path.join(path, fname)\n",
    "                if sys.version_info < (3,):\n",
    "                    f = open(fpath)\n",
    "                else:\n",
    "                    f = open(fpath, encoding='latin-1')\n",
    "                t = f.read()\n",
    "                i = t.find('\\n\\n')  # 把每个文本文件中头部信息，非新闻信息删掉了。\n",
    "                if 0 < i:\n",
    "                    t = t[i:]\n",
    "                texts.append(t)\n",
    "                f.close()\n",
    "                labels.append(label_id)\n",
    "\n",
    "print('Found %s texts.' % len(texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对样本数据texts进行词分割放入列表sequences中，并保留其中最常见的20000个词。即sequences中每个元素是一个新闻分词后的列表。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1528\n",
      "19997\n",
      "[1213, 2632, 5, 11, 41, 176, 173, 4, 930, 2050]\n"
     ]
    }
   ],
   "source": [
    "###把文本训练样本数据和标签转换为词向量矩阵。(19997,1)->(50, 19997)\n",
    "MAX_NB_WORDS = 20000\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# 类实例化\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS) ##num_words保留最常见的词数量，这里是20000\n",
    "# texts要用以训练的文本列表,喂入文本数据\n",
    "tokenizer.fit_on_texts(texts)\n",
    "# \n",
    "sequences = tokenizer.texts_to_sequences(texts) #返回：序列的列表，列表中每个序列对应于一段输入文本\n",
    "print(len(sequences[0])) ## 第一个新闻有多少个词\n",
    "print(len(sequences))  ## 19997个样本sequences\n",
    "print(sequences[0][-10:]) # 将文本中每个词转换为其对应在字典中的索引"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们用另一个类方法句子分割，来对比看一下第一个新闻中词的数量，发现直接句子分割要上面的方法要多，这是因为不常见的被去掉了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1733\n",
      "['archive', 'name', 'atheism', 'resources', 'alt', 'atheism', 'archive', 'name', 'resources', 'last']\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "sequences_0 = text_to_word_sequence(texts[0]) ## 句子分割\n",
    "print(len(sequences_0))\n",
    "print(sequences_0[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们发现name对应273, archive对应1237.　为什么1733>1528，因为只保留了最常见的20000个词，故对于第一个句子，有部分词被删除了吧。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tokenizer的一个属性word_index，用来查看训练样本每个词以及他们的索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 174074 unique tokens.\n",
      "273\n"
     ]
    }
   ],
   "source": [
    "### hash table\n",
    "### word_index是训练样本中每个词以及其对应的索引。\n",
    "word_index = tokenizer.word_index  ### 字典，将单词（字符串）映射为它们的排名或者索引。仅在调用fit_on_texts之后设置。\n",
    "print(\"Found %s unique tokens.\" % len(word_index))\n",
    "print(word_index['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19997, 2000)\n",
      "[   0    0    0 ...,    4  930 2050]\n"
     ]
    }
   ],
   "source": [
    "MAX_SEQUENCE_LENGTH = 2000 ## 每篇新闻只取前2000个词\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH) ##　不足2000词的padding\n",
    "\n",
    "print(data.shape)\n",
    "print(data[0,:]) # 从开头padding的啊"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19997\n",
      "shape of data tensor: (19997, 2000)\n",
      "shape of label tensor: (19997, 20)\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "\n",
    "print(len(labels))\n",
    "labels = to_categorical(np.asarray(labels))  ##　总共有20类，通过计算可得到。\n",
    "print(\"shape of data tensor:\", data.shape)\n",
    "print(\"shape of label tensor:\", labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into a training set and a validation set\n",
    "VALIDATION_SPILT = 0.01\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "nb_validation_samples = int(VALIDATION_SPILT * data.shape[0]) ## 0.01\n",
    "\n",
    "x_train = data[:-nb_validation_samples]\n",
    "y_train = labels[:-nb_validation_samples]\n",
    "x_val = data[-nb_validation_samples:]\n",
    "y_val = labels[-nb_validation_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19798, 20)\n",
      "(199, 20)\n"
     ]
    }
   ],
   "source": [
    "print(y_train.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preparing the Embedding layer\n",
    "接下来，我们从GloVe文件中解析出每个词和它所对应的词向量，并用字典的方式存储"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n",
      "[ 0.52042001 -0.83139998  0.49961001  1.28929996  0.1151      0.057521\n",
      " -1.37530005 -0.97312999  0.18346     0.47672001 -0.15112001  0.35532001\n",
      "  0.25911999 -0.77857     0.52181     0.47694999 -1.42509997  0.85799998\n",
      "  0.59820998 -1.09029996  0.33574    -0.60891002  0.41742     0.21569\n",
      " -0.07417    -0.58219999 -0.45019999  0.17253     0.16448    -0.38413\n",
      "  2.3283     -0.66681999 -0.58181     0.74388999  0.095015   -0.47865\n",
      " -0.84591001  0.38703999  0.23693    -1.55229998  0.64802003 -0.16520999\n",
      " -1.47189999 -0.16224     0.79856998  0.97390997  0.40026999 -0.21912\n",
      " -0.30937999  0.26581001]\n"
     ]
    }
   ],
   "source": [
    "### 从50d的glove词向量获取词向量map\n",
    "embeddings_index = {}\n",
    "GLOVE_DIR = '/home/panxie/Documents/cs231n/myNotes_rnn/keras_models'\n",
    "with open(os.path.join(GLOVE_DIR, 'glove.6B.50d.txt')) as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print(\"Found %s word vectors.\" % len(embeddings_index)) \n",
    "print(embeddings_index['apple'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(174075, 50)\n"
     ]
    }
   ],
   "source": [
    "## 根据得到的字典生成上文所定义的词向量矩阵\n",
    "EMBEDDING_DIM = embeddings_index['apple'].shape[0]\n",
    "embedding_matrix = np.zeros((len(word_index)+1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word) #将word_index中每个词换成其在GLOVE词向量中对应的向量\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector ##嵌入到训练样本对应的词向量中去\n",
    "print(embedding_matrix.shape) ## 174075 < 19997　说明有些词在GLOVE里面没有。。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n",
    "\n",
    "embedding_layer = Embedding(input_dim=len(word_index)+1, output_dim=EMBEDDING_DIM,\n",
    "                           weights=[embedding_matrix],\n",
    "                           input_length=MAX_SEQUENCE_LENGTH,\n",
    "                           trainable=False)\n",
    "##对于Embedding层\n",
    "## input: (174075, 2000)\n",
    "## output: (174075, 2000, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一个Embedding层的输入应该是一系列的整数序列，比如一个2D的输入，它的shape值为(samples, indices)，也就是一个samples行，indeces列的矩阵。每一次的batch训练的输入应该被padded成相同大小（尽管Embedding层有能力处理不定长序列，如果你不指定数列长度这一参数） dim). 所有的序列中的整数都将被对应的词向量矩阵中对应的列（也就是它的词向量）代替,比如序列[1,2]将被序列[词向量[1],词向量[2]]代替。这样，输入一个2D张量后，我们可以得到一个3D张量(samples, sequence_length, embedding_dim).。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a 1D convnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19798 samples, validate on 199 samples\n",
      "Epoch 1/10\n",
      "19798/19798 [==============================] - 266s 13ms/step - loss: 2.2871 - acc: 0.2398 - val_loss: 1.8404 - val_acc: 0.3618\n",
      "Epoch 2/10\n",
      "19798/19798 [==============================] - 246s 12ms/step - loss: 1.5756 - acc: 0.4495 - val_loss: 1.6407 - val_acc: 0.4925\n",
      "Epoch 3/10\n",
      "19798/19798 [==============================] - 270s 14ms/step - loss: 1.2733 - acc: 0.5606 - val_loss: 1.3420 - val_acc: 0.5528\n",
      "Epoch 4/10\n",
      "19798/19798 [==============================] - 276s 14ms/step - loss: 1.0905 - acc: 0.6289 - val_loss: 1.0659 - val_acc: 0.6683\n",
      "Epoch 5/10\n",
      "19798/19798 [==============================] - 264s 13ms/step - loss: 0.9793 - acc: 0.6727 - val_loss: 1.1185 - val_acc: 0.6533\n",
      "Epoch 6/10\n",
      "19798/19798 [==============================] - 239s 12ms/step - loss: 0.8754 - acc: 0.7057 - val_loss: 1.1205 - val_acc: 0.6784\n",
      "Epoch 7/10\n",
      "19798/19798 [==============================] - 240s 12ms/step - loss: 0.7963 - acc: 0.7308 - val_loss: 1.0837 - val_acc: 0.6633\n",
      "Epoch 8/10\n",
      "19798/19798 [==============================] - 242s 12ms/step - loss: 0.7161 - acc: 0.7603 - val_loss: 0.9945 - val_acc: 0.7186\n",
      "Epoch 9/10\n",
      "19798/19798 [==============================] - 255s 13ms/step - loss: 0.6443 - acc: 0.7817 - val_loss: 1.4556 - val_acc: 0.5779\n",
      "Epoch 10/10\n",
      "19798/19798 [==============================] - 239s 12ms/step - loss: 0.6037 - acc: 0.8008 - val_loss: 0.9001 - val_acc: 0.7538\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7ec8ad3358>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import Dense, Input, GlobalMaxPooling1D\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.models import Model\n",
    "\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32') #(2000,None)\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "x = Conv1D(128, 5, activation='relu')(embedded_sequences)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Conv1D(128, 5, activation='relu')(x)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Conv1D(128, 5, activation='relu')(x)\n",
    "x = GlobalMaxPooling1D()(x)  ## global max pooling\n",
    "x = Dense(128, activation='relu')(x)\n",
    "preds = Dense(len(labels_index), activation='softmax')(x)\n",
    "\n",
    "model = Model(sequence_input, preds)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n",
    "\n",
    "model.fit(x_train,y_train,\n",
    "         batch_size=128,\n",
    "         epochs=10,\n",
    "         validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
