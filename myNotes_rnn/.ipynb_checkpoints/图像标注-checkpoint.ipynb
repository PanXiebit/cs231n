{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Captioning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Explain Images with Multimodal Recurrent Neural Networks, Mao et al.]\n",
    "\n",
    "[Deep Visual-Semantic Alignments for Generating Image Descriptions, Karpathy and Fei-Fei]\n",
    "\n",
    "[Show and Tell: A Neural Image Caption Generator, Vinyals et al.]\n",
    "\n",
    "[Long-term Recurrent Convolutional Networks for Visual Recognition and Description, Donahue et al.]\n",
    "\n",
    "[Learning a Recurrent Visual Representation for Image Caption Generation, Chen and Zitnick]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://img.blog.csdn.net/20180308171345853?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcGFueGlhb3hpZQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70\" weight=200 height=200>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "图像标注：由CNN和RNN组成。CNN用来处理图像，生成图像的特征向量，然后输入到循环神经网络语言模型的第一个时序。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![这里写图片描述](http://img.blog.csdn.net/20180308173042655?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcGFueGlhb3hpZQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![这里写图片描述](http://img.blog.csdn.net/20180308172727908?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcGFueGlhb3hpZQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将CNN生成的特征向量V输入到RNN的第一个时序,与`<START>`词向量通过计算，以及softmax得到y0.通过y0在字典中采样得到下一个词的输入，依次循环整个时间序列，直到得到`<END>`.\n",
    "\n",
    "**有点疑惑？**因为之前已经确定sentences的长度，如果在这个长度内没有生成`<END>`，那应该就截断吧？\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from builtins import range\n",
    "from builtins import object\n",
    "import numpy as np\n",
    "\n",
    "from cs231n.layers import *\n",
    "from cs231n.rnn_layers import *\n",
    "\n",
    "\n",
    "class CaptioningRNN(object):\n",
    "    \"\"\"\n",
    "    A CaptioningRNN produces captions from image features using a recurrent\n",
    "    neural network.\n",
    "\n",
    "    The RNN receives input vectors of size D, has a vocab size of V, works on\n",
    "    sequences of length T, has an RNN hidden dimension of H, uses word vectors\n",
    "    of dimension W, and operates on minibatches of size N.\n",
    "\n",
    "    Note that we don't use any regularization for the CaptioningRNN.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, word_to_idx, input_dim=512, wordvec_dim=128,\n",
    "                 hidden_dim=128, cell_type='rnn', dtype=np.float32):\n",
    "        \"\"\"\n",
    "        Construct a new CaptioningRNN instance.\n",
    "\n",
    "        Inputs:\n",
    "        - word_to_idx: A dictionary giving the vocabulary. It contains V entries,\n",
    "          and maps each string to a unique integer in the range [0, V).\n",
    "        - input_dim: Dimension D of input image feature vectors.\n",
    "        - wordvec_dim: Dimension W of word vectors.\n",
    "        - hidden_dim: Dimension H for the hidden state of the RNN.\n",
    "        - cell_type: What type of RNN to use; either 'rnn' or 'lstm'.\n",
    "        - dtype: numpy datatype to use; use float32 for training and float64 for\n",
    "          numeric gradient checking.\n",
    "        \"\"\"\n",
    "        if cell_type not in {'rnn', 'lstm'}:\n",
    "            raise ValueError('Invalid cell_type \"%s\"' % cell_type)\n",
    "\n",
    "        self.cell_type = cell_type\n",
    "        self.dtype = dtype\n",
    "        self.word_to_idx = word_to_idx\n",
    "        self.idx_to_word = {i: w for w, i in word_to_idx.items()}\n",
    "        self.params = {}\n",
    "\n",
    "        vocab_size = len(word_to_idx)\n",
    "\n",
    "        self._null = word_to_idx['<NULL>']\n",
    "        self._start = word_to_idx.get('<START>', None)\n",
    "        self._end = word_to_idx.get('<END>', None)\n",
    "\n",
    "        # Initialize word vectors\n",
    "        self.params['W_embed'] = np.random.randn(vocab_size, wordvec_dim)\n",
    "        self.params['W_embed'] /= 100\n",
    "\n",
    "        # Initialize CNN -> hidden state projection parameters\n",
    "        self.params['W_proj'] = np.random.randn(input_dim, hidden_dim)\n",
    "        self.params['W_proj'] /= np.sqrt(input_dim)\n",
    "        self.params['b_proj'] = np.zeros(hidden_dim)\n",
    "\n",
    "        # Initialize parameters for the RNN\n",
    "        dim_mul = {'lstm': 4, 'rnn': 1}[cell_type]\n",
    "        self.params['Wx'] = np.random.randn(wordvec_dim, dim_mul * hidden_dim)\n",
    "        self.params['Wx'] /= np.sqrt(wordvec_dim)\n",
    "        self.params['Wh'] = np.random.randn(hidden_dim, dim_mul * hidden_dim)\n",
    "        self.params['Wh'] /= np.sqrt(hidden_dim)\n",
    "        self.params['b'] = np.zeros(dim_mul * hidden_dim)\n",
    "\n",
    "        # Initialize output to vocab weights\n",
    "        self.params['W_vocab'] = np.random.randn(hidden_dim, vocab_size)\n",
    "        self.params['W_vocab'] /= np.sqrt(hidden_dim)\n",
    "        self.params['b_vocab'] = np.zeros(vocab_size)\n",
    "\n",
    "        # Cast parameters to correct dtype\n",
    "        for k, v in self.params.items():\n",
    "            self.params[k] = v.astype(self.dtype)\n",
    "\n",
    "\n",
    "    def loss(self, features, captions):\n",
    "        \"\"\"\n",
    "        Compute training-time loss for the RNN. We input image features and\n",
    "        ground-truth captions for those images, and use an RNN (or LSTM) to compute\n",
    "        loss and gradients on all parameters.\n",
    "\n",
    "        Inputs:\n",
    "        - features: Input image features, of shape (N, D)\n",
    "        - captions: Ground-truth captions; an integer array of shape (N, T) where\n",
    "          each element is in the range 0 <= y[i, t] < V\n",
    "\n",
    "        Returns a tuple of:\n",
    "        - loss: Scalar loss\n",
    "        - grads: Dictionary of gradients parallel to self.params\n",
    "        \"\"\"\n",
    "        # Cut captions into two pieces: captions_in has everything but the last word\n",
    "        # and will be input to the RNN; captions_out has everything but the first\n",
    "        # word and this is what we will expect the RNN to generate. These are offset\n",
    "        # by one relative to each other because the RNN should produce word (t+1)\n",
    "        # after receiving word t. The first element of captions_in will be the START\n",
    "        # token, and the first element of captions_out will be the first word.\n",
    "        captions_in = captions[:, :-1]\n",
    "        captions_out = captions[:, 1:]\n",
    "\n",
    "        # You'll need this\n",
    "        mask = (captions_out != self._null)\n",
    "\n",
    "        # Weight and bias for the affine transform from image features to initial\n",
    "        # hidden state\n",
    "        W_proj, b_proj = self.params['W_proj'], self.params['b_proj']\n",
    "\n",
    "        # Word embedding matrix\n",
    "        W_embed = self.params['W_embed']\n",
    "\n",
    "        # Input-to-hidden, hidden-to-hidden, and biases for the RNN\n",
    "        Wx, Wh, b = self.params['Wx'], self.params['Wh'], self.params['b']\n",
    "\n",
    "        # Weight and bias for the hidden-to-vocab transformation.\n",
    "        W_vocab, b_vocab = self.params['W_vocab'], self.params['b_vocab']\n",
    "\n",
    "        loss, grads = 0.0, {}\n",
    "        ############################################################################\n",
    "        # TODO: Implement the forward and backward passes for the CaptioningRNN.   #\n",
    "        # In the forward pass you will need to do the following:                   #\n",
    "        # (1) Use an affine transformation to compute the initial hidden state     #\n",
    "        #     from the image features. This should produce an array of shape (N, H)#\n",
    "        # (2) Use a word embedding layer to transform the words in captions_in     #\n",
    "        #     from indices to vectors, giving an array of shape (N, T, W).         #\n",
    "        # (3) Use either a vanilla RNN or LSTM (depending on self.cell_type) to    #\n",
    "        #     process the sequence of input word vectors and produce hidden state  #\n",
    "        #     vectors for all timesteps, producing an array of shape (N, T, H).    #\n",
    "        # (4) Use a (temporal) affine transformation to compute scores over the    #\n",
    "        #     vocabulary at every timestep using the hidden states, giving an      #\n",
    "        #     array of shape (N, T, V).                                            #\n",
    "        # (5) Use (temporal) softmax to compute loss using captions_out, ignoring  #\n",
    "        #     the points where the output word is <NULL> using the mask above.     #\n",
    "        #                                                                          #\n",
    "        # In the backward pass you will need to compute the gradient of the loss   #\n",
    "        # with respect to all model parameters. Use the loss and grads variables   #\n",
    "        # defined above to store loss and gradients; grads[k] should give the      #\n",
    "        # gradients for self.params[k].                                            #\n",
    "        ############################################################################\n",
    "        pass\n",
    "        ############################################################################\n",
    "        #                             END OF YOUR CODE                             #\n",
    "        ############################################################################\n",
    "\n",
    "        return loss, grads\n",
    "\n",
    "\n",
    "    def sample(self, features, max_length=30):\n",
    "        \"\"\"\n",
    "        Run a test-time forward pass for the model, sampling captions for input\n",
    "        feature vectors.\n",
    "\n",
    "        At each timestep, we embed the current word, pass it and the previous hidden\n",
    "        state to the RNN to get the next hidden state, use the hidden state to get\n",
    "        scores for all vocab words, and choose the word with the highest score as\n",
    "        the next word. The initial hidden state is computed by applying an affine\n",
    "        transform to the input image features, and the initial word is the <START>\n",
    "        token.\n",
    "\n",
    "        For LSTMs you will also have to keep track of the cell state; in that case\n",
    "        the initial cell state should be zero.\n",
    "\n",
    "        Inputs:\n",
    "        - features: Array of input image features of shape (N, D).\n",
    "        - max_length: Maximum length T of generated captions.\n",
    "\n",
    "        Returns:\n",
    "        - captions: Array of shape (N, max_length) giving sampled captions,\n",
    "          where each element is an integer in the range [0, V). The first element\n",
    "          of captions should be the first sampled word, not the <START> token.\n",
    "        \"\"\"\n",
    "        N = features.shape[0]\n",
    "        captions = self._null * np.ones((N, max_length), dtype=np.int32)\n",
    "\n",
    "        # Unpack parameters\n",
    "        W_proj, b_proj = self.params['W_proj'], self.params['b_proj']\n",
    "        W_embed = self.params['W_embed']\n",
    "        Wx, Wh, b = self.params['Wx'], self.params['Wh'], self.params['b']\n",
    "        W_vocab, b_vocab = self.params['W_vocab'], self.params['b_vocab']\n",
    "\n",
    "        ###########################################################################\n",
    "        # TODO: Implement test-time sampling for the model. You will need to      #\n",
    "        # initialize the hidden state of the RNN by applying the learned affine   #\n",
    "        # transform to the input image features. The first word that you feed to  #\n",
    "        # the RNN should be the <START> token; its value is stored in the         #\n",
    "        # variable self._start. At each timestep you will need to do to:          #\n",
    "        # (1) Embed the previous word using the learned word embeddings           #\n",
    "        # (2) Make an RNN step using the previous hidden state and the embedded   #\n",
    "        #     current word to get the next hidden state.                          #\n",
    "        # (3) Apply the learned affine transformation to the next hidden state to #\n",
    "        #     get scores for all words in the vocabulary                          #\n",
    "        # (4) Select the word with the highest score as the next word, writing it #\n",
    "        #     to the appropriate slot in the captions variable                    #\n",
    "        #                                                                         #\n",
    "        # For simplicity, you do not need to stop generating after an <END> token #\n",
    "        # is sampled, but you can if you want to.                                 #\n",
    "        #                                                                         #\n",
    "        # HINT: You will not be able to use the rnn_forward or lstm_forward       #\n",
    "        # functions; you'll need to call rnn_step_forward or lstm_step_forward in #\n",
    "        # a loop.                                                                 #\n",
    "        ###########################################################################\n",
    "        pass\n",
    "        ############################################################################\n",
    "        #                             END OF YOUR CODE                             #\n",
    "        ############################################################################\n",
    "        return captions\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
